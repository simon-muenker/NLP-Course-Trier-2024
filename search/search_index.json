{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started Usage Install pip install cltrier_nlp Development Install The project is managed by Poetry, a dependency management and packaging library. Please set up a local version according to the official installation guidelines . When finished, install the local repository as follows: # install package dependencies poetry install # add pre-commit to git hooks poetry run pre-commit install Tests poetry run pytest Linting poetry run pre-commit run --all-files Project Structure \u2502 \u251c\u2500\u2500 Makefile <- Makefile containing development targets \u251c\u2500\u2500 README.md <- top-level README \u251c\u2500\u2500 pyproject.toml <- package-level (poetry) configuration \u251c\u2500\u2500 mkdocs.yaml <- documentation configuration \u251c\u2500\u2500 .pre-commit-config.yaml <- git pre-commit actions \u2502 \u251c\u2500\u2500 cltrier_nlp <- root source \u2502 \u2514\u2500\u2500 corpus <- nltk inspired corpus module \u2502 \u2514\u2500\u2500 encoder <- huggingface auto model wrapper \u2502 \u2514\u2500\u2500 trainer <- pytorch training algorithm \u2502 \u2514\u2500\u2500 functional <- generic helper functions \u2502 \u2514\u2500\u2500 utility <- utility classes and types \u2502 \u251c\u2500\u2500 tests <- unittests \u2502 \u251c\u2500\u2500 examples <- usage/application examples \u2502 \u251c\u2500\u2500 scripts <- additional package building scripts \u2502 \u2514\u2500\u2500 gen_docs_pages.py <- automatic doc generation based on docstrings \u2502 ToDos [ ] tests: add encoder testing [ ] tests: add functional testing [ ] tests: add utility testing [ ] cltrier_nlp:trainer: modernize and refactor [ ] examples:application: encoder with manifold reduction [ ] examples:application: encoder with unsupervised clustering [ ] examples:application: training pipeline with pytorch MLP Resources Project Template (Data Science): https://github.com/drivendata/cookiecutter-data-science Project Template (Poetry): https://github.com/fpgmaas/cookiecutter-poetry Sitemap corpus sentence encoder batch pooler functional neural text trainer metric progress utility map types","title":"Getting started"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#usage","text":"","title":"Usage"},{"location":"#install","text":"pip install cltrier_nlp","title":"Install"},{"location":"#development","text":"","title":"Development"},{"location":"#install_1","text":"The project is managed by Poetry, a dependency management and packaging library. Please set up a local version according to the official installation guidelines . When finished, install the local repository as follows: # install package dependencies poetry install # add pre-commit to git hooks poetry run pre-commit install","title":"Install"},{"location":"#tests","text":"poetry run pytest","title":"Tests"},{"location":"#linting","text":"poetry run pre-commit run --all-files","title":"Linting"},{"location":"#project-structure","text":"\u2502 \u251c\u2500\u2500 Makefile <- Makefile containing development targets \u251c\u2500\u2500 README.md <- top-level README \u251c\u2500\u2500 pyproject.toml <- package-level (poetry) configuration \u251c\u2500\u2500 mkdocs.yaml <- documentation configuration \u251c\u2500\u2500 .pre-commit-config.yaml <- git pre-commit actions \u2502 \u251c\u2500\u2500 cltrier_nlp <- root source \u2502 \u2514\u2500\u2500 corpus <- nltk inspired corpus module \u2502 \u2514\u2500\u2500 encoder <- huggingface auto model wrapper \u2502 \u2514\u2500\u2500 trainer <- pytorch training algorithm \u2502 \u2514\u2500\u2500 functional <- generic helper functions \u2502 \u2514\u2500\u2500 utility <- utility classes and types \u2502 \u251c\u2500\u2500 tests <- unittests \u2502 \u251c\u2500\u2500 examples <- usage/application examples \u2502 \u251c\u2500\u2500 scripts <- additional package building scripts \u2502 \u2514\u2500\u2500 gen_docs_pages.py <- automatic doc generation based on docstrings \u2502","title":"Project Structure"},{"location":"#todos","text":"[ ] tests: add encoder testing [ ] tests: add functional testing [ ] tests: add utility testing [ ] cltrier_nlp:trainer: modernize and refactor [ ] examples:application: encoder with manifold reduction [ ] examples:application: encoder with unsupervised clustering [ ] examples:application: training pipeline with pytorch MLP","title":"ToDos"},{"location":"#resources","text":"Project Template (Data Science): https://github.com/drivendata/cookiecutter-data-science Project Template (Poetry): https://github.com/fpgmaas/cookiecutter-poetry","title":"Resources"},{"location":"#sitemap","text":"corpus sentence encoder batch pooler functional neural text trainer metric progress utility map types","title":"Sitemap"},{"location":"corpus/","text":"Corpus Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Corpus ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str sentences : typing . List [ Sentence ] = [] args : CorpusArgs = CorpusArgs () @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] @pydantic . computed_field # type: ignore[misc] @property def tokens ( self ) -> utility . types . Tokens : \"\"\" Returns: \"\"\" return [ tok for sent in self . sentences for tok in sent . tokens ] @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 5 ) def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n )) def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), # type: ignore[has-type] ) def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ()) def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences ) bigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: pentagram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: tetragram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: tokens : utility . types . Tokens property Returns: trigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: __len__ () Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/__init__.py 185 186 187 188 189 190 191 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences ) count_languages () Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 100 101 102 103 104 105 106 def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) count_ngrams ( n ) Parameters: n ( int ) \u2013 Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 123 124 125 126 127 128 129 130 131 132 def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n )) count_tokens () Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) create_subset_by_language ( language ) Parameters: language ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), # type: ignore[has-type] ) from_txt ( path ) classmethod Parameters: path ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 163 164 165 166 167 168 169 170 171 172 173 @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ()) generate_ngrams ( n ) Parameters: n ( int ) \u2013 Returns: NGrams \u2013 utility.types.NGrams: Source code in cltrier_nlp/corpus/__init__.py 150 151 152 153 154 155 156 157 158 159 160 161 def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] model_post_init ( __context ) Parameters: __context ( Any ) \u2013 ?? Returns: Source code in cltrier_nlp/corpus/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] to_df () Returns: DataFrame \u2013 pandas.DataFrame: Source code in cltrier_nlp/corpus/__init__.py 175 176 177 178 179 180 181 182 183 def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) CorpusArgs Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 18 19 20 21 22 23 24 class CorpusArgs ( pydantic . BaseModel ): \"\"\" \"\"\" token_count_exclude : utility . types . Tokens = pydantic . Field ( default_factory = lambda : [ \"\u2019\" , \"\u201c\" , \"\u201d\" , * string . punctuation ] ) Sentence Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) bigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: pentagram : typing . List [ typing . Tuple [ str , ... ]] property Returns: tetragram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: trigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: __len__ () Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) model_post_init ( __context ) Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) to_row () Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"Corpus"},{"location":"corpus/#cltrier_nlp.corpus.Corpus","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Corpus ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str sentences : typing . List [ Sentence ] = [] args : CorpusArgs = CorpusArgs () @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] @pydantic . computed_field # type: ignore[misc] @property def tokens ( self ) -> utility . types . Tokens : \"\"\" Returns: \"\"\" return [ tok for sent in self . sentences for tok in sent . tokens ] @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 5 ) def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n )) def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), # type: ignore[has-type] ) def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ()) def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences )","title":"Corpus"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.bigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"bigrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.pentagram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"pentagram"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.tetragram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"tetragram"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.tokens","text":"Returns:","title":"tokens"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.trigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"trigrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.__len__","text":"Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/__init__.py 185 186 187 188 189 190 191 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences )","title":"__len__"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_languages","text":"Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 100 101 102 103 104 105 106 def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ])","title":"count_languages"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_ngrams","text":"Parameters: n ( int ) \u2013 Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 123 124 125 126 127 128 129 130 131 132 def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n ))","title":"count_ngrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_tokens","text":"Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ])","title":"count_tokens"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.create_subset_by_language","text":"Parameters: language ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), # type: ignore[has-type] )","title":"create_subset_by_language"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.from_txt","text":"Parameters: path ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 163 164 165 166 167 168 169 170 171 172 173 @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ())","title":"from_txt"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.generate_ngrams","text":"Parameters: n ( int ) \u2013 Returns: NGrams \u2013 utility.types.NGrams: Source code in cltrier_nlp/corpus/__init__.py 150 151 152 153 154 155 156 157 158 159 160 161 def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ]","title":"generate_ngrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.model_post_init","text":"Parameters: __context ( Any ) \u2013 ?? Returns: Source code in cltrier_nlp/corpus/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ]","title":"model_post_init"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.to_df","text":"Returns: DataFrame \u2013 pandas.DataFrame: Source code in cltrier_nlp/corpus/__init__.py 175 176 177 178 179 180 181 182 183 def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], )","title":"to_df"},{"location":"corpus/#cltrier_nlp.corpus.CorpusArgs","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 18 19 20 21 22 23 24 class CorpusArgs ( pydantic . BaseModel ): \"\"\" \"\"\" token_count_exclude : utility . types . Tokens = pydantic . Field ( default_factory = lambda : [ \"\u2019\" , \"\u201c\" , \"\u201d\" , * string . punctuation ] )","title":"CorpusArgs"},{"location":"corpus/#cltrier_nlp.corpus.Sentence","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"Sentence"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.bigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"bigrams"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.pentagram","text":"Returns:","title":"pentagram"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.tetragram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"tetragram"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.trigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"trigrams"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.__len__","text":"Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"__len__"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.model_post_init","text":"Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw )","title":"model_post_init"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.to_row","text":"Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"to_row"},{"location":"corpus/sentence/","text":"Sentence Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) bigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: pentagram : typing . List [ typing . Tuple [ str , ... ]] property Returns: tetragram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: trigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: __len__ () Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) model_post_init ( __context ) Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) to_row () Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"Sentence"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"Sentence"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.bigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"bigrams"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.pentagram","text":"Returns:","title":"pentagram"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.tetragram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"tetragram"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.trigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"trigrams"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.__len__","text":"Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"__len__"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.model_post_init","text":"Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw )","title":"model_post_init"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.to_row","text":"Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"to_row"},{"location":"encoder/","text":"Encoder Bases: Module Source code in cltrier_nlp/encoder/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class Encoder ( torch . nn . Module ): \"\"\" \"\"\" @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), embeds = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), token = [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], unpad = unpad , ) def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) @property def dim ( self ) -> int : \"\"\" Return the dimension of the model. \"\"\" return self . model . config . to_dict ()[ \"hidden_size\" ] def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" ) dim : int property Return the dimension of the model. __call__ ( batch , unpad = False ) Tokenizes input batch and returns embeddings and tokens with optional padding removal. Parameters: batch ( Batch [ str ] ) \u2013 List of input strings to be tokenized. unpad ( bool , default: False ) \u2013 Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch ( EncoderBatch ) \u2013 A EncodedBatch object. See documentation for more. Source code in cltrier_nlp/encoder/__init__.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), embeds = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), token = [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], unpad = unpad , ) __init__ ( args = EncoderArgs ()) Initialize the encoder with the provided EncoderConfig. Parameters: args ( EncoderArgs , default: EncoderArgs () ) \u2013 The configuration for the encoder. Source code in cltrier_nlp/encoder/__init__.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) __len__ () Return the length of the object based on the vocabulary size. Source code in cltrier_nlp/encoder/__init__.py 128 129 130 131 132 def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] __repr__ () Return a string representation of the encoder including memory usage. Source code in cltrier_nlp/encoder/__init__.py 134 135 136 137 138 139 140 141 def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" ) forward ( ids , masks ) Perform a forward pass through the model and return the aggregated hidden states. Parameters: ids ( Tensor ) \u2013 The input tensor for token ids. masks ( Tensor ) \u2013 The input tensor for attention masks. Returns: Tensor \u2013 torch.Tensor: The aggregated hidden states obtained from the model's forward pass. Source code in cltrier_nlp/encoder/__init__.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) ids_to_sent ( ids ) Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Parameters: ids ( Tensor ) \u2013 The input tensor of token IDs. Returns: str ( str ) \u2013 The decoded string output. Source code in cltrier_nlp/encoder/__init__.py 109 110 111 112 113 114 115 116 117 118 119 def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) ids_to_tokens ( ids ) Convert the input token IDs to a list of token strings using the internal tokenizer. Parameters: ids ( Tensor ) \u2013 The input token IDs to be converted to tokens. Returns: List [ str ] \u2013 List[str]: A list of token strings corresponding to the input token IDs. Source code in cltrier_nlp/encoder/__init__.py 97 98 99 100 101 102 103 104 105 106 107 def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) EncoderArgs Bases: BaseModel Source code in cltrier_nlp/encoder/__init__.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderArgs ( pydantic . BaseModel ): \"\"\" \"\"\" model : str = \"prajjwal1/bert-tiny\" layers : typing . List [ int ] = [ - 1 ] device : str = functional . neural . get_device () tokenizer : typing . Dict [ str , str | int ] = dict ( max_length = 512 , truncation = True , return_offsets_mapping = True , ) EncoderBatch Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . Union [ torch . Tensor , utility . types . Batch [ torch . Tensor ]] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )] EncoderPooler Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ]) __call__ ( encodes , extract_spans = None , form = EncoderPoolerArgs () . types ) Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"Encoder"},{"location":"encoder/#cltrier_nlp.encoder.Encoder","text":"Bases: Module Source code in cltrier_nlp/encoder/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class Encoder ( torch . nn . Module ): \"\"\" \"\"\" @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), embeds = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), token = [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], unpad = unpad , ) def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) @property def dim ( self ) -> int : \"\"\" Return the dimension of the model. \"\"\" return self . model . config . to_dict ()[ \"hidden_size\" ] def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" )","title":"Encoder"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.dim","text":"Return the dimension of the model.","title":"dim"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__call__","text":"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Parameters: batch ( Batch [ str ] ) \u2013 List of input strings to be tokenized. unpad ( bool , default: False ) \u2013 Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch ( EncoderBatch ) \u2013 A EncodedBatch object. See documentation for more. Source code in cltrier_nlp/encoder/__init__.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), embeds = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), token = [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], unpad = unpad , )","title":"__call__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__init__","text":"Initialize the encoder with the provided EncoderConfig. Parameters: args ( EncoderArgs , default: EncoderArgs () ) \u2013 The configuration for the encoder. Source code in cltrier_nlp/encoder/__init__.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self )","title":"__init__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__len__","text":"Return the length of the object based on the vocabulary size. Source code in cltrier_nlp/encoder/__init__.py 128 129 130 131 132 def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ]","title":"__len__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__repr__","text":"Return a string representation of the encoder including memory usage. Source code in cltrier_nlp/encoder/__init__.py 134 135 136 137 138 139 140 141 def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" )","title":"__repr__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.forward","text":"Perform a forward pass through the model and return the aggregated hidden states. Parameters: ids ( Tensor ) \u2013 The input tensor for token ids. masks ( Tensor ) \u2013 The input tensor for attention masks. Returns: Tensor \u2013 torch.Tensor: The aggregated hidden states obtained from the model's forward pass. Source code in cltrier_nlp/encoder/__init__.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () )","title":"forward"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.ids_to_sent","text":"Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Parameters: ids ( Tensor ) \u2013 The input tensor of token IDs. Returns: str ( str ) \u2013 The decoded string output. Source code in cltrier_nlp/encoder/__init__.py 109 110 111 112 113 114 115 116 117 118 119 def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True )","title":"ids_to_sent"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.ids_to_tokens","text":"Convert the input token IDs to a list of token strings using the internal tokenizer. Parameters: ids ( Tensor ) \u2013 The input token IDs to be converted to tokens. Returns: List [ str ] \u2013 List[str]: A list of token strings corresponding to the input token IDs. Source code in cltrier_nlp/encoder/__init__.py 97 98 99 100 101 102 103 104 105 106 107 def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids )","title":"ids_to_tokens"},{"location":"encoder/#cltrier_nlp.encoder.EncoderArgs","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/__init__.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderArgs ( pydantic . BaseModel ): \"\"\" \"\"\" model : str = \"prajjwal1/bert-tiny\" layers : typing . List [ int ] = [ - 1 ] device : str = functional . neural . get_device () tokenizer : typing . Dict [ str , str | int ] = dict ( max_length = 512 , truncation = True , return_offsets_mapping = True , )","title":"EncoderArgs"},{"location":"encoder/#cltrier_nlp.encoder.EncoderBatch","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . Union [ torch . Tensor , utility . types . Batch [ torch . Tensor ]] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"EncoderBatch"},{"location":"encoder/#cltrier_nlp.encoder.EncoderPooler","text":"Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ])","title":"EncoderPooler"},{"location":"encoder/#cltrier_nlp.encoder.EncoderPooler.__call__","text":"Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"__call__"},{"location":"encoder/batch/","text":"EncoderBatch Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . Union [ torch . Tensor , utility . types . Batch [ torch . Tensor ]] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"Batch"},{"location":"encoder/batch/#cltrier_nlp.encoder.batch.EncoderBatch","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . Union [ torch . Tensor , utility . types . Batch [ torch . Tensor ]] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"EncoderBatch"},{"location":"encoder/pooler/","text":"EncoderPooler Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ]) __call__ ( encodes , extract_spans = None , form = EncoderPoolerArgs () . types ) Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] EncoderPoolerArgs Bases: BaseModel Source code in cltrier_nlp/encoder/pooler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EncoderPoolerArgs ( pydantic . BaseModel ): \"\"\" \"\"\" fns : typing . Dict [ str , typing . Callable ] = { # sentence based \"sent_cls\" : lambda x : x [ 0 ], \"sent_mean\" : lambda x : torch . mean ( x [ 1 : - 1 ], dim = 0 ), # word based, positional extraction \"subword_first\" : lambda x : x [ 0 ], \"subword_last\" : lambda x : x [ - 1 ], # word based, arithmetic extraction \"subword_mean\" : lambda x : torch . mean ( x , dim = 0 ), \"subword_min\" : lambda x : torch . min ( x , dim = 0 )[ 0 ], \"subword_max\" : lambda x : torch . max ( x , dim = 0 )[ 0 ], } types : typing . Literal [ \"sent_cls\" , \"sent_mean\" , \"subword_first\" , \"subword_last\" , \"subword_mean\" , \"subword_min\" , \"subword_max\" , ] = \"sent_cls\"","title":"Pooler"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPooler","text":"Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ])","title":"EncoderPooler"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPooler.__call__","text":"Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( list ( encodes . embeds ) if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"__call__"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPoolerArgs","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/pooler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EncoderPoolerArgs ( pydantic . BaseModel ): \"\"\" \"\"\" fns : typing . Dict [ str , typing . Callable ] = { # sentence based \"sent_cls\" : lambda x : x [ 0 ], \"sent_mean\" : lambda x : torch . mean ( x [ 1 : - 1 ], dim = 0 ), # word based, positional extraction \"subword_first\" : lambda x : x [ 0 ], \"subword_last\" : lambda x : x [ - 1 ], # word based, arithmetic extraction \"subword_mean\" : lambda x : torch . mean ( x , dim = 0 ), \"subword_min\" : lambda x : torch . min ( x , dim = 0 )[ 0 ], \"subword_max\" : lambda x : torch . max ( x , dim = 0 )[ 0 ], } types : typing . Literal [ \"sent_cls\" , \"sent_mean\" , \"subword_first\" , \"subword_last\" , \"subword_mean\" , \"subword_min\" , \"subword_max\" , ] = \"sent_cls\"","title":"EncoderPoolerArgs"},{"location":"functional/","text":"This functional collection aims to improve code usability, readability, and overall development efficiency. They can be integrated into educational and research projects. timeit ( func ) Decorator function to measure the execution time of the function argument. Parameters: func ( Callable ) \u2013 The function to be measured. Returns: any ( Callable ) \u2013 The result of the input function. Source code in cltrier_nlp/functional/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def timeit ( func : typing . Callable ) -> typing . Callable : \"\"\" Decorator function to measure the execution time of the function argument. Args: func (Callable): The function to be measured. Returns: any: The result of the input function. \"\"\" @functools . wraps ( func ) def wrap ( * args , ** kwargs ) -> typing . Any : \"\"\" Decorator function that wraps the input function, measures its execution time, logs the result, and returns the result. \"\"\" start = time . time () result = func ( * args , ** kwargs ) logging . info ( f \"> f( { func . __name__ } ) took: { time . time () - start : 2.4f } sec\" ) return result return wrap","title":"Functional"},{"location":"functional/#cltrier_nlp.functional.timeit","text":"Decorator function to measure the execution time of the function argument. Parameters: func ( Callable ) \u2013 The function to be measured. Returns: any ( Callable ) \u2013 The result of the input function. Source code in cltrier_nlp/functional/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def timeit ( func : typing . Callable ) -> typing . Callable : \"\"\" Decorator function to measure the execution time of the function argument. Args: func (Callable): The function to be measured. Returns: any: The result of the input function. \"\"\" @functools . wraps ( func ) def wrap ( * args , ** kwargs ) -> typing . Any : \"\"\" Decorator function that wraps the input function, measures its execution time, logs the result, and returns the result. \"\"\" start = time . time () result = func ( * args , ** kwargs ) logging . info ( f \"> f( { func . __name__ } ) took: { time . time () - start : 2.4f } sec\" ) return result return wrap","title":"timeit"},{"location":"functional/neural/","text":"calculate_model_memory_usage ( model ) Calculate the memory usage of the input model in megabytes. Parameters: model ( Module ) \u2013 The input model for which memory usage needs to be calculated. Returns: str ( str ) \u2013 A string formatted to represent the size of the nn.Module in megabytes. Source code in cltrier_nlp/functional/neural.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def calculate_model_memory_usage ( model : torch . nn . Module ) -> str : \"\"\"Calculate the memory usage of the input model in megabytes. Args: model (torch.nn.Module): The input model for which memory usage needs to be calculated. Returns: str: A string formatted to represent the size of the nn.Module in megabytes. \"\"\" usage_in_byte : int = sum ( [ sum ([ param . nelement () * param . element_size () for param in model . parameters ()]), sum ([ buf . nelement () * buf . element_size () for buf in model . buffers ()]), ] ) return f \" { usage_in_byte / ( 1024.0 * 1024.0 ) : 2.4f } MB\" get_device () Return the computation device as a string based on the availability of CUDA. Returns: str ( str ) \u2013 A PyTorch device object representing the appropriate device for computation. Source code in cltrier_nlp/functional/neural.py 4 5 6 7 8 9 10 def get_device () -> str : \"\"\"Return the computation device as a string based on the availability of CUDA. Returns: str: A PyTorch device object representing the appropriate device for computation. \"\"\" return \"cuda\" if torch . cuda . is_available () else \"cpu\"","title":"Neural"},{"location":"functional/neural/#cltrier_nlp.functional.neural.calculate_model_memory_usage","text":"Calculate the memory usage of the input model in megabytes. Parameters: model ( Module ) \u2013 The input model for which memory usage needs to be calculated. Returns: str ( str ) \u2013 A string formatted to represent the size of the nn.Module in megabytes. Source code in cltrier_nlp/functional/neural.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def calculate_model_memory_usage ( model : torch . nn . Module ) -> str : \"\"\"Calculate the memory usage of the input model in megabytes. Args: model (torch.nn.Module): The input model for which memory usage needs to be calculated. Returns: str: A string formatted to represent the size of the nn.Module in megabytes. \"\"\" usage_in_byte : int = sum ( [ sum ([ param . nelement () * param . element_size () for param in model . parameters ()]), sum ([ buf . nelement () * buf . element_size () for buf in model . buffers ()]), ] ) return f \" { usage_in_byte / ( 1024.0 * 1024.0 ) : 2.4f } MB\"","title":"calculate_model_memory_usage"},{"location":"functional/neural/#cltrier_nlp.functional.neural.get_device","text":"Return the computation device as a string based on the availability of CUDA. Returns: str ( str ) \u2013 A PyTorch device object representing the appropriate device for computation. Source code in cltrier_nlp/functional/neural.py 4 5 6 7 8 9 10 def get_device () -> str : \"\"\"Return the computation device as a string based on the availability of CUDA. Returns: str: A PyTorch device object representing the appropriate device for computation. \"\"\" return \"cuda\" if torch . cuda . is_available () else \"cpu\"","title":"get_device"},{"location":"functional/text/","text":"detect_language ( content ) Parameters: content ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def detect_language ( content : str ) -> str : \"\"\" Args: content: Returns: \"\"\" # Ignore langcodes dependent language data warning # DeprecationWarning: pkg_resources is deprecated as an API. with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) try : return langcodes . Language . get ( langdetect . detect ( content )) . display_name () . lower () except langdetect . lang_detect_exception . LangDetectException : return UNK_LANG load_stopwords ( languages ) Parameters: languages ( Tokens ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def load_stopwords ( languages : utility . types . Tokens ) -> utility . types . Tokens : \"\"\" Args: languages: Returns: \"\"\" return list ( set () . union ( * [ nltk . corpus . stopwords . words ( lang ) for lang in languages if lang in nltk . corpus . stopwords . fileids () ] ) ) ngrams ( tokens , n ) Parameters: tokens ( Tokens ) \u2013 n ( int ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 60 61 62 63 64 65 66 67 68 69 70 def ngrams ( tokens : utility . types . Tokens , n : int ) -> utility . types . NGrams : \"\"\" Args: tokens: n: Returns: \"\"\" return [ tuple ( tokens [ i : i + n ]) for i in range ( len ( tokens ) - n + 1 )] sentenize ( text ) Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 32 33 34 35 36 37 38 39 40 41 def sentenize ( text : str ) -> utility . types . Batch [ str ]: \"\"\" Args: text: Returns: \"\"\" return nltk . tokenize . sent_tokenize ( text , language = detect_language ( text )) tokenize ( text ) Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def tokenize ( text : str ) -> utility . types . Tokens : \"\"\" Args: text: Returns: \"\"\" try : return nltk . tokenize . word_tokenize ( text . lower (), language = detect_language ( text )) except LookupError : return nltk . tokenize . word_tokenize ( text . lower ())","title":"Text"},{"location":"functional/text/#cltrier_nlp.functional.text.detect_language","text":"Parameters: content ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def detect_language ( content : str ) -> str : \"\"\" Args: content: Returns: \"\"\" # Ignore langcodes dependent language data warning # DeprecationWarning: pkg_resources is deprecated as an API. with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) try : return langcodes . Language . get ( langdetect . detect ( content )) . display_name () . lower () except langdetect . lang_detect_exception . LangDetectException : return UNK_LANG","title":"detect_language"},{"location":"functional/text/#cltrier_nlp.functional.text.load_stopwords","text":"Parameters: languages ( Tokens ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def load_stopwords ( languages : utility . types . Tokens ) -> utility . types . Tokens : \"\"\" Args: languages: Returns: \"\"\" return list ( set () . union ( * [ nltk . corpus . stopwords . words ( lang ) for lang in languages if lang in nltk . corpus . stopwords . fileids () ] ) )","title":"load_stopwords"},{"location":"functional/text/#cltrier_nlp.functional.text.ngrams","text":"Parameters: tokens ( Tokens ) \u2013 n ( int ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 60 61 62 63 64 65 66 67 68 69 70 def ngrams ( tokens : utility . types . Tokens , n : int ) -> utility . types . NGrams : \"\"\" Args: tokens: n: Returns: \"\"\" return [ tuple ( tokens [ i : i + n ]) for i in range ( len ( tokens ) - n + 1 )]","title":"ngrams"},{"location":"functional/text/#cltrier_nlp.functional.text.sentenize","text":"Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 32 33 34 35 36 37 38 39 40 41 def sentenize ( text : str ) -> utility . types . Batch [ str ]: \"\"\" Args: text: Returns: \"\"\" return nltk . tokenize . sent_tokenize ( text , language = detect_language ( text ))","title":"sentenize"},{"location":"functional/text/#cltrier_nlp.functional.text.tokenize","text":"Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def tokenize ( text : str ) -> utility . types . Tokens : \"\"\" Args: text: Returns: \"\"\" try : return nltk . tokenize . word_tokenize ( text . lower (), language = detect_language ( text )) except LookupError : return nltk . tokenize . word_tokenize ( text . lower ())","title":"tokenize"},{"location":"trainer/","text":"Trainer Bases: BaseModel Source code in cltrier_nlp/trainer/__init__.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class Trainer ( pydantic . BaseModel ): dataset : typing . Dict [ str , torch_data . Dataset ] model : torch . nn . Module collation_fn : typing . Callable label_decoding_fn : typing . Callable args : TrainerArgs = TrainerArgs () progress : Progress = Progress () def model_post_init ( self , __context : typing . Any ): \"\"\" Initialize data loaders and load optimizer for the model. \"\"\" # create data loaders self . data_loader : typing . Dict [ str , torch_data . DataLoader ] = { label : torch_data . DataLoader ( dataset , shuffle = True , drop_last = True , batch_size = self . args . batch_size , collate_fn = self . collation_fn ) for label , dataset in self . dataset . items () } # load optimizer self . optimizer = torch . optim . AdamW ( self . model . parameters (), lr = self . args . learning_rate ) def __call__ ( self ) -> None : \"\"\" Execute the training process for a specified number of epochs. If the training process is interrupted by the user, it will skip to the evaluation step if possible. \"\"\" try : for epoch in range ( self . args . num_epochs ): self . _epoch () self . progress . log () if self . progress . last_is_best : self . model . save_pretrained ( self . args . export_path ) except KeyboardInterrupt : logging . warning ( 'Warning: Training interrupted by user, skipping to evaluation if possible.' ) if self . progress : self . _evaluate () self . progress . export ( f ' { self . args . export_path } /metric.train' ) def _epoch ( self ) -> None : \"\"\" Method to perform a single epoch of training and testing, and record the progress. \"\"\" time_begin : datetime . datetime = datetime . datetime . now () self . progress . append_record ( epoch = len ( self . progress . epoch ) + 1 , duration = datetime . datetime . now () - time_begin , train_results = self . _step ( self . data_loader [ 'train' ], optimize = True ), test_results = self . _step ( self . data_loader [ 'test' ]) ) def _step ( self , data_loader : torch_data . DataLoader , optimize : bool = False ) -> typing . Tuple [ float , float , typing . Dict [ str , pd . Series ]]: \"\"\" Perform a step of the training process. Args: data_loader (torch.utils.data.DataLoader): The data loader for the training data. optimize (bool, optional): Whether to optimize the model. Defaults to False. Returns: Tuple[float, float, Dict[str, pd.Series]]: A tuple containing the average loss value, the F score metric, and a dictionary of metric data. \"\"\" loss_value : float = 0.0 metric = Metric ( decoding_fn = self . label_decoding_fn ) for batch in tqdm . tqdm ( data_loader , leave = False ): loss = self . _forward ( batch , metric ) loss_value += loss . item () if optimize : self . _optimize ( loss ) del loss return loss_value / len ( data_loader ), metric . f_score (), metric . data def _forward ( self , batch : typing . Dict , metric : Metric ) -> torch . Tensor : \"\"\" Compute the forward pass through the model and update the metric with observations. Args: batch (dict): The input data batch. metric (Metric): The metric object for tracking performance. Returns: torch.Tensor: The loss value from the forward pass. \"\"\" predictions , loss = self . model ( ** batch ) metric . add_observations ( pd . Series ( batch [ 'labels' ] . cpu () . numpy ()), pd . Series ( torch . argmax ( predictions , dim = 1 ) . cpu () . numpy ()) ) return loss def _optimize ( self , loss : torch . Tensor ) -> None : \"\"\" Optimize the model by performing a backward pass to compute gradients, then taking a step with the optimizer and zeroing the gradients. Args: loss (torch.Tensor): The loss value to perform backpropagation. \"\"\" loss . backward () self . optimizer . step () self . optimizer . zero_grad () def _evaluate ( self ) -> None : \"\"\" Evaluate the model using the max value of f1_test and export the metric to the specified export_path. \"\"\" logging . info ( '[--- EVALUATION on max(f1_test) ---]' ) metric = Metric ( decoding_fn = self . label_decoding_fn , ** self . progress . metric_test [ self . progress . max_record_id ] ) metric . export ( self . args . export_path ) logging . info ( metric ) __call__ () Execute the training process for a specified number of epochs. If the training process is interrupted by the user, it will skip to the evaluation step if possible. Source code in cltrier_nlp/trainer/__init__.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __call__ ( self ) -> None : \"\"\" Execute the training process for a specified number of epochs. If the training process is interrupted by the user, it will skip to the evaluation step if possible. \"\"\" try : for epoch in range ( self . args . num_epochs ): self . _epoch () self . progress . log () if self . progress . last_is_best : self . model . save_pretrained ( self . args . export_path ) except KeyboardInterrupt : logging . warning ( 'Warning: Training interrupted by user, skipping to evaluation if possible.' ) if self . progress : self . _evaluate () self . progress . export ( f ' { self . args . export_path } /metric.train' ) model_post_init ( __context ) Initialize data loaders and load optimizer for the model. Source code in cltrier_nlp/trainer/__init__.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def model_post_init ( self , __context : typing . Any ): \"\"\" Initialize data loaders and load optimizer for the model. \"\"\" # create data loaders self . data_loader : typing . Dict [ str , torch_data . DataLoader ] = { label : torch_data . DataLoader ( dataset , shuffle = True , drop_last = True , batch_size = self . args . batch_size , collate_fn = self . collation_fn ) for label , dataset in self . dataset . items () } # load optimizer self . optimizer = torch . optim . AdamW ( self . model . parameters (), lr = self . args . learning_rate )","title":"Trainer"},{"location":"trainer/#cltrier_nlp.trainer.Trainer","text":"Bases: BaseModel Source code in cltrier_nlp/trainer/__init__.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class Trainer ( pydantic . BaseModel ): dataset : typing . Dict [ str , torch_data . Dataset ] model : torch . nn . Module collation_fn : typing . Callable label_decoding_fn : typing . Callable args : TrainerArgs = TrainerArgs () progress : Progress = Progress () def model_post_init ( self , __context : typing . Any ): \"\"\" Initialize data loaders and load optimizer for the model. \"\"\" # create data loaders self . data_loader : typing . Dict [ str , torch_data . DataLoader ] = { label : torch_data . DataLoader ( dataset , shuffle = True , drop_last = True , batch_size = self . args . batch_size , collate_fn = self . collation_fn ) for label , dataset in self . dataset . items () } # load optimizer self . optimizer = torch . optim . AdamW ( self . model . parameters (), lr = self . args . learning_rate ) def __call__ ( self ) -> None : \"\"\" Execute the training process for a specified number of epochs. If the training process is interrupted by the user, it will skip to the evaluation step if possible. \"\"\" try : for epoch in range ( self . args . num_epochs ): self . _epoch () self . progress . log () if self . progress . last_is_best : self . model . save_pretrained ( self . args . export_path ) except KeyboardInterrupt : logging . warning ( 'Warning: Training interrupted by user, skipping to evaluation if possible.' ) if self . progress : self . _evaluate () self . progress . export ( f ' { self . args . export_path } /metric.train' ) def _epoch ( self ) -> None : \"\"\" Method to perform a single epoch of training and testing, and record the progress. \"\"\" time_begin : datetime . datetime = datetime . datetime . now () self . progress . append_record ( epoch = len ( self . progress . epoch ) + 1 , duration = datetime . datetime . now () - time_begin , train_results = self . _step ( self . data_loader [ 'train' ], optimize = True ), test_results = self . _step ( self . data_loader [ 'test' ]) ) def _step ( self , data_loader : torch_data . DataLoader , optimize : bool = False ) -> typing . Tuple [ float , float , typing . Dict [ str , pd . Series ]]: \"\"\" Perform a step of the training process. Args: data_loader (torch.utils.data.DataLoader): The data loader for the training data. optimize (bool, optional): Whether to optimize the model. Defaults to False. Returns: Tuple[float, float, Dict[str, pd.Series]]: A tuple containing the average loss value, the F score metric, and a dictionary of metric data. \"\"\" loss_value : float = 0.0 metric = Metric ( decoding_fn = self . label_decoding_fn ) for batch in tqdm . tqdm ( data_loader , leave = False ): loss = self . _forward ( batch , metric ) loss_value += loss . item () if optimize : self . _optimize ( loss ) del loss return loss_value / len ( data_loader ), metric . f_score (), metric . data def _forward ( self , batch : typing . Dict , metric : Metric ) -> torch . Tensor : \"\"\" Compute the forward pass through the model and update the metric with observations. Args: batch (dict): The input data batch. metric (Metric): The metric object for tracking performance. Returns: torch.Tensor: The loss value from the forward pass. \"\"\" predictions , loss = self . model ( ** batch ) metric . add_observations ( pd . Series ( batch [ 'labels' ] . cpu () . numpy ()), pd . Series ( torch . argmax ( predictions , dim = 1 ) . cpu () . numpy ()) ) return loss def _optimize ( self , loss : torch . Tensor ) -> None : \"\"\" Optimize the model by performing a backward pass to compute gradients, then taking a step with the optimizer and zeroing the gradients. Args: loss (torch.Tensor): The loss value to perform backpropagation. \"\"\" loss . backward () self . optimizer . step () self . optimizer . zero_grad () def _evaluate ( self ) -> None : \"\"\" Evaluate the model using the max value of f1_test and export the metric to the specified export_path. \"\"\" logging . info ( '[--- EVALUATION on max(f1_test) ---]' ) metric = Metric ( decoding_fn = self . label_decoding_fn , ** self . progress . metric_test [ self . progress . max_record_id ] ) metric . export ( self . args . export_path ) logging . info ( metric )","title":"Trainer"},{"location":"trainer/#cltrier_nlp.trainer.Trainer.__call__","text":"Execute the training process for a specified number of epochs. If the training process is interrupted by the user, it will skip to the evaluation step if possible. Source code in cltrier_nlp/trainer/__init__.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __call__ ( self ) -> None : \"\"\" Execute the training process for a specified number of epochs. If the training process is interrupted by the user, it will skip to the evaluation step if possible. \"\"\" try : for epoch in range ( self . args . num_epochs ): self . _epoch () self . progress . log () if self . progress . last_is_best : self . model . save_pretrained ( self . args . export_path ) except KeyboardInterrupt : logging . warning ( 'Warning: Training interrupted by user, skipping to evaluation if possible.' ) if self . progress : self . _evaluate () self . progress . export ( f ' { self . args . export_path } /metric.train' )","title":"__call__"},{"location":"trainer/#cltrier_nlp.trainer.Trainer.model_post_init","text":"Initialize data loaders and load optimizer for the model. Source code in cltrier_nlp/trainer/__init__.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def model_post_init ( self , __context : typing . Any ): \"\"\" Initialize data loaders and load optimizer for the model. \"\"\" # create data loaders self . data_loader : typing . Dict [ str , torch_data . DataLoader ] = { label : torch_data . DataLoader ( dataset , shuffle = True , drop_last = True , batch_size = self . args . batch_size , collate_fn = self . collation_fn ) for label , dataset in self . dataset . items () } # load optimizer self . optimizer = torch . optim . AdamW ( self . model . parameters (), lr = self . args . learning_rate )","title":"model_post_init"},{"location":"trainer/metric/","text":"Metric Bases: BaseModel Source code in cltrier_nlp/trainer/metric.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class Metric ( pydantic . BaseModel ): golds : pd . Series = pydantic . Field ( default_factory = lambda : pd . Series ) preds : pd . Series = pydantic . Field ( default_factory = lambda : pd . Series ) decoding_fn : typing . Callable = lambda x : x def add_observations ( self , golds : pd . Series , preds : pd . Series ): \"\"\" Adds new observations to the existing golds and preds Series. Args: golds (pd.Series): The new gold observations to add. preds (pd.Series): The new predicted observations to add. \"\"\" self . golds = golds if self . golds . empty else pd . concat ([ self . golds , golds ], ignore_index = True ) self . preds = preds if self . preds . empty else pd . concat ([ self . preds , preds ], ignore_index = True ) def f_score ( self , reduce : str = \"weighted\" ) -> float : \"\"\" Calculate the F1 score for the given gold standard and predicted values. Args: reduce (str): The reduction method for the F1 score calculation. Defaults to \"weighted\". Returns: float: The F1 score. \"\"\" return sk_metrics . f1_score ( self . golds , self . preds , average = reduce , zero_division = 0.0 ) def accuracy ( self ) -> float : \"\"\" Returns: float: The accuracy score. \"\"\" return sk_metrics . accuracy_score ( self . golds , self . preds ) def cross_tabulation ( self ) -> pd . DataFrame : \"\"\" Returns a pandas DataFrame containing the cross-tabulation of gold and predicted values. \"\"\" return pd . crosstab ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), rownames = [ 'gold' ], colnames = [ 'pred' ] ) def classification_report ( self ) -> pd . DataFrame : \"\"\" Return a classification report as a pandas DataFrame. Returns: pd.DataFrame: The classification report as a pandas DataFrame. \"\"\" return pd . DataFrame ( sk_metrics . classification_report ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), zero_division = 0.0 , output_dict = True ) ) . T . drop ( 'accuracy' ) def export ( self , path : str ) -> None : \"\"\" Export the classification report and cross tabulation to CSV files. Args: path (str): The path where the CSV files will be exported. \"\"\" self . classification_report () . to_csv ( f ' { path } /metric.classification_report.csv' ) self . cross_tabulation () . to_csv ( f ' { path } /metric.cross_tabulation.csv' ) @pydantic . computed_field # type: ignore[misc] @property def data ( self ) -> typing . Dict [ str , pd . Series ]: return { 'golds' : self . golds , 'preds' : self . preds } def __repr__ ( self ) -> str : \"\"\" Return a string representation of the classification report as a dictionary. \"\"\" return self . classification_report () . __repr__ () __repr__ () Return a string representation of the classification report as a dictionary. Source code in cltrier_nlp/trainer/metric.py 87 88 89 90 91 def __repr__ ( self ) -> str : \"\"\" Return a string representation of the classification report as a dictionary. \"\"\" return self . classification_report () . __repr__ () accuracy () Returns: float ( float ) \u2013 The accuracy score. Source code in cltrier_nlp/trainer/metric.py 37 38 39 40 41 42 43 def accuracy ( self ) -> float : \"\"\" Returns: float: The accuracy score. \"\"\" return sk_metrics . accuracy_score ( self . golds , self . preds ) add_observations ( golds , preds ) Adds new observations to the existing golds and preds Series. Parameters: golds ( Series ) \u2013 The new gold observations to add. preds ( Series ) \u2013 The new predicted observations to add. Source code in cltrier_nlp/trainer/metric.py 14 15 16 17 18 19 20 21 22 23 def add_observations ( self , golds : pd . Series , preds : pd . Series ): \"\"\" Adds new observations to the existing golds and preds Series. Args: golds (pd.Series): The new gold observations to add. preds (pd.Series): The new predicted observations to add. \"\"\" self . golds = golds if self . golds . empty else pd . concat ([ self . golds , golds ], ignore_index = True ) self . preds = preds if self . preds . empty else pd . concat ([ self . preds , preds ], ignore_index = True ) classification_report () Return a classification report as a pandas DataFrame. Returns: DataFrame \u2013 pd.DataFrame: The classification report as a pandas DataFrame. Source code in cltrier_nlp/trainer/metric.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def classification_report ( self ) -> pd . DataFrame : \"\"\" Return a classification report as a pandas DataFrame. Returns: pd.DataFrame: The classification report as a pandas DataFrame. \"\"\" return pd . DataFrame ( sk_metrics . classification_report ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), zero_division = 0.0 , output_dict = True ) ) . T . drop ( 'accuracy' ) cross_tabulation () Returns a pandas DataFrame containing the cross-tabulation of gold and predicted values. Source code in cltrier_nlp/trainer/metric.py 45 46 47 48 49 50 51 52 53 54 def cross_tabulation ( self ) -> pd . DataFrame : \"\"\" Returns a pandas DataFrame containing the cross-tabulation of gold and predicted values. \"\"\" return pd . crosstab ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), rownames = [ 'gold' ], colnames = [ 'pred' ] ) export ( path ) Export the classification report and cross tabulation to CSV files. Parameters: path ( str ) \u2013 The path where the CSV files will be exported. Source code in cltrier_nlp/trainer/metric.py 72 73 74 75 76 77 78 79 80 def export ( self , path : str ) -> None : \"\"\" Export the classification report and cross tabulation to CSV files. Args: path (str): The path where the CSV files will be exported. \"\"\" self . classification_report () . to_csv ( f ' { path } /metric.classification_report.csv' ) self . cross_tabulation () . to_csv ( f ' { path } /metric.cross_tabulation.csv' ) f_score ( reduce = 'weighted' ) Calculate the F1 score for the given gold standard and predicted values. Parameters: reduce ( str , default: 'weighted' ) \u2013 The reduction method for the F1 score calculation. Defaults to \"weighted\". Returns: float ( float ) \u2013 The F1 score. Source code in cltrier_nlp/trainer/metric.py 25 26 27 28 29 30 31 32 33 34 35 def f_score ( self , reduce : str = \"weighted\" ) -> float : \"\"\" Calculate the F1 score for the given gold standard and predicted values. Args: reduce (str): The reduction method for the F1 score calculation. Defaults to \"weighted\". Returns: float: The F1 score. \"\"\" return sk_metrics . f1_score ( self . golds , self . preds , average = reduce , zero_division = 0.0 )","title":"Metric"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric","text":"Bases: BaseModel Source code in cltrier_nlp/trainer/metric.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class Metric ( pydantic . BaseModel ): golds : pd . Series = pydantic . Field ( default_factory = lambda : pd . Series ) preds : pd . Series = pydantic . Field ( default_factory = lambda : pd . Series ) decoding_fn : typing . Callable = lambda x : x def add_observations ( self , golds : pd . Series , preds : pd . Series ): \"\"\" Adds new observations to the existing golds and preds Series. Args: golds (pd.Series): The new gold observations to add. preds (pd.Series): The new predicted observations to add. \"\"\" self . golds = golds if self . golds . empty else pd . concat ([ self . golds , golds ], ignore_index = True ) self . preds = preds if self . preds . empty else pd . concat ([ self . preds , preds ], ignore_index = True ) def f_score ( self , reduce : str = \"weighted\" ) -> float : \"\"\" Calculate the F1 score for the given gold standard and predicted values. Args: reduce (str): The reduction method for the F1 score calculation. Defaults to \"weighted\". Returns: float: The F1 score. \"\"\" return sk_metrics . f1_score ( self . golds , self . preds , average = reduce , zero_division = 0.0 ) def accuracy ( self ) -> float : \"\"\" Returns: float: The accuracy score. \"\"\" return sk_metrics . accuracy_score ( self . golds , self . preds ) def cross_tabulation ( self ) -> pd . DataFrame : \"\"\" Returns a pandas DataFrame containing the cross-tabulation of gold and predicted values. \"\"\" return pd . crosstab ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), rownames = [ 'gold' ], colnames = [ 'pred' ] ) def classification_report ( self ) -> pd . DataFrame : \"\"\" Return a classification report as a pandas DataFrame. Returns: pd.DataFrame: The classification report as a pandas DataFrame. \"\"\" return pd . DataFrame ( sk_metrics . classification_report ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), zero_division = 0.0 , output_dict = True ) ) . T . drop ( 'accuracy' ) def export ( self , path : str ) -> None : \"\"\" Export the classification report and cross tabulation to CSV files. Args: path (str): The path where the CSV files will be exported. \"\"\" self . classification_report () . to_csv ( f ' { path } /metric.classification_report.csv' ) self . cross_tabulation () . to_csv ( f ' { path } /metric.cross_tabulation.csv' ) @pydantic . computed_field # type: ignore[misc] @property def data ( self ) -> typing . Dict [ str , pd . Series ]: return { 'golds' : self . golds , 'preds' : self . preds } def __repr__ ( self ) -> str : \"\"\" Return a string representation of the classification report as a dictionary. \"\"\" return self . classification_report () . __repr__ ()","title":"Metric"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.__repr__","text":"Return a string representation of the classification report as a dictionary. Source code in cltrier_nlp/trainer/metric.py 87 88 89 90 91 def __repr__ ( self ) -> str : \"\"\" Return a string representation of the classification report as a dictionary. \"\"\" return self . classification_report () . __repr__ ()","title":"__repr__"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.accuracy","text":"Returns: float ( float ) \u2013 The accuracy score. Source code in cltrier_nlp/trainer/metric.py 37 38 39 40 41 42 43 def accuracy ( self ) -> float : \"\"\" Returns: float: The accuracy score. \"\"\" return sk_metrics . accuracy_score ( self . golds , self . preds )","title":"accuracy"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.add_observations","text":"Adds new observations to the existing golds and preds Series. Parameters: golds ( Series ) \u2013 The new gold observations to add. preds ( Series ) \u2013 The new predicted observations to add. Source code in cltrier_nlp/trainer/metric.py 14 15 16 17 18 19 20 21 22 23 def add_observations ( self , golds : pd . Series , preds : pd . Series ): \"\"\" Adds new observations to the existing golds and preds Series. Args: golds (pd.Series): The new gold observations to add. preds (pd.Series): The new predicted observations to add. \"\"\" self . golds = golds if self . golds . empty else pd . concat ([ self . golds , golds ], ignore_index = True ) self . preds = preds if self . preds . empty else pd . concat ([ self . preds , preds ], ignore_index = True )","title":"add_observations"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.classification_report","text":"Return a classification report as a pandas DataFrame. Returns: DataFrame \u2013 pd.DataFrame: The classification report as a pandas DataFrame. Source code in cltrier_nlp/trainer/metric.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def classification_report ( self ) -> pd . DataFrame : \"\"\" Return a classification report as a pandas DataFrame. Returns: pd.DataFrame: The classification report as a pandas DataFrame. \"\"\" return pd . DataFrame ( sk_metrics . classification_report ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), zero_division = 0.0 , output_dict = True ) ) . T . drop ( 'accuracy' )","title":"classification_report"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.cross_tabulation","text":"Returns a pandas DataFrame containing the cross-tabulation of gold and predicted values. Source code in cltrier_nlp/trainer/metric.py 45 46 47 48 49 50 51 52 53 54 def cross_tabulation ( self ) -> pd . DataFrame : \"\"\" Returns a pandas DataFrame containing the cross-tabulation of gold and predicted values. \"\"\" return pd . crosstab ( self . golds . apply ( self . decoding_fn ), self . preds . apply ( self . decoding_fn ), rownames = [ 'gold' ], colnames = [ 'pred' ] )","title":"cross_tabulation"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.export","text":"Export the classification report and cross tabulation to CSV files. Parameters: path ( str ) \u2013 The path where the CSV files will be exported. Source code in cltrier_nlp/trainer/metric.py 72 73 74 75 76 77 78 79 80 def export ( self , path : str ) -> None : \"\"\" Export the classification report and cross tabulation to CSV files. Args: path (str): The path where the CSV files will be exported. \"\"\" self . classification_report () . to_csv ( f ' { path } /metric.classification_report.csv' ) self . cross_tabulation () . to_csv ( f ' { path } /metric.cross_tabulation.csv' )","title":"export"},{"location":"trainer/metric/#cltrier_nlp.trainer.metric.Metric.f_score","text":"Calculate the F1 score for the given gold standard and predicted values. Parameters: reduce ( str , default: 'weighted' ) \u2013 The reduction method for the F1 score calculation. Defaults to \"weighted\". Returns: float ( float ) \u2013 The F1 score. Source code in cltrier_nlp/trainer/metric.py 25 26 27 28 29 30 31 32 33 34 35 def f_score ( self , reduce : str = \"weighted\" ) -> float : \"\"\" Calculate the F1 score for the given gold standard and predicted values. Args: reduce (str): The reduction method for the F1 score calculation. Defaults to \"weighted\". Returns: float: The F1 score. \"\"\" return sk_metrics . f1_score ( self . golds , self . preds , average = reduce , zero_division = 0.0 )","title":"f_score"},{"location":"trainer/progress/","text":"Progress Bases: BaseModel Source code in cltrier_nlp/trainer/progress.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class Progress ( pydantic . BaseModel ): epoch : typing . List [ int ] = [] duration : typing . List [ datetime . timedelta ] = [] loss_train : typing . List [ float ] = [] loss_test : typing . List [ float ] = [] f1_train : typing . List [ float ] = [] f1_test : typing . List [ float ] = [] metric_train : typing . List [ dict ] = [] metric_test : typing . List [ dict ] = [] def append_record ( self , epoch : int , duration : datetime . timedelta , train_results : typing . Tuple [ float , float , dict ], test_results : typing . Tuple [ float , float , dict ] ): \"\"\" Append a record of the epoch, duration, train results, and test results to their respective lists. Args: epoch (int): The epoch number. duration (datetime.timedelta): The duration of the training. train_results (typing.Tuple[float, float, dict]): The results of the training. test_results (typing.Tuple[float, float, dict]): The results of the testing. \"\"\" self . epoch . append ( epoch ) self . duration . append ( duration ) self . loss_train . append ( train_results [ 0 ]) self . loss_test . append ( test_results [ 0 ]) self . f1_train . append ( train_results [ 1 ]) self . f1_test . append ( test_results [ 1 ]) self . metric_train . append ( train_results [ 2 ]) self . metric_test . append ( test_results [ 2 ]) @pydantic . computed_field # type: ignore[misc] @property def max_record_id ( self ) -> int : \"\"\" Returns the maximum record id as an integer. \"\"\" if not self . f1_test : return - 1 return self . f1_test . index ( max ( self . f1_test )) @pydantic . computed_field # type: ignore[misc] @property def last_is_best ( self ) -> bool : \"\"\" Returns a boolean indicating whether the last element in f1_test is the maximum. \"\"\" if not self . f1_test : return False return self . f1_test [ - 1 ] == max ( self . f1_test ) def log ( self ) -> None : \"\"\" Logs the training and testing metrics, including loss and F1 scores, along with the epoch number and duration. \"\"\" logging . info (( f '[@ { self . epoch [ - 1 ] : 03 } ]: \\t ' f 'loss_train= { self . loss_train [ - 1 ] : 2.4f } \\t ' f 'loss_test= { self . loss_test [ - 1 ] : 2.4f } \\t ' f 'f1_train= { self . f1_train [ - 1 ] : 2.4f } \\t ' f 'f1_test= { self . f1_test [ - 1 ] : 2.4f } \\t ' f 'duration= { self . duration [ - 1 ] } ' )) def export ( self , path : str ) -> None : \"\"\" Exports the record dump to a CSV file at the specified path. Args: path (str): The file path to export the CSV. \"\"\" ( pd . DataFrame . from_records ( self . model_dump ( exclude = { 'metric_train' , 'metric_test' } ), index = [ 'epoch' ]) . to_csv ( f ' { path } .csv' ) ) last_is_best : bool property Returns a boolean indicating whether the last element in f1_test is the maximum. max_record_id : int property Returns the maximum record id as an integer. append_record ( epoch , duration , train_results , test_results ) Append a record of the epoch, duration, train results, and test results to their respective lists. Parameters: epoch ( int ) \u2013 The epoch number. duration ( timedelta ) \u2013 The duration of the training. train_results ( Tuple [ float , float , dict ] ) \u2013 The results of the training. test_results ( Tuple [ float , float , dict ] ) \u2013 The results of the testing. Source code in cltrier_nlp/trainer/progress.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def append_record ( self , epoch : int , duration : datetime . timedelta , train_results : typing . Tuple [ float , float , dict ], test_results : typing . Tuple [ float , float , dict ] ): \"\"\" Append a record of the epoch, duration, train results, and test results to their respective lists. Args: epoch (int): The epoch number. duration (datetime.timedelta): The duration of the training. train_results (typing.Tuple[float, float, dict]): The results of the training. test_results (typing.Tuple[float, float, dict]): The results of the testing. \"\"\" self . epoch . append ( epoch ) self . duration . append ( duration ) self . loss_train . append ( train_results [ 0 ]) self . loss_test . append ( test_results [ 0 ]) self . f1_train . append ( train_results [ 1 ]) self . f1_test . append ( test_results [ 1 ]) self . metric_train . append ( train_results [ 2 ]) self . metric_test . append ( test_results [ 2 ]) export ( path ) Exports the record dump to a CSV file at the specified path. Parameters: path ( str ) \u2013 The file path to export the CSV. Source code in cltrier_nlp/trainer/progress.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def export ( self , path : str ) -> None : \"\"\" Exports the record dump to a CSV file at the specified path. Args: path (str): The file path to export the CSV. \"\"\" ( pd . DataFrame . from_records ( self . model_dump ( exclude = { 'metric_train' , 'metric_test' } ), index = [ 'epoch' ]) . to_csv ( f ' { path } .csv' ) ) log () Logs the training and testing metrics, including loss and F1 scores, along with the epoch number and duration. Source code in cltrier_nlp/trainer/progress.py 73 74 75 76 77 78 79 80 81 82 83 84 def log ( self ) -> None : \"\"\" Logs the training and testing metrics, including loss and F1 scores, along with the epoch number and duration. \"\"\" logging . info (( f '[@ { self . epoch [ - 1 ] : 03 } ]: \\t ' f 'loss_train= { self . loss_train [ - 1 ] : 2.4f } \\t ' f 'loss_test= { self . loss_test [ - 1 ] : 2.4f } \\t ' f 'f1_train= { self . f1_train [ - 1 ] : 2.4f } \\t ' f 'f1_test= { self . f1_test [ - 1 ] : 2.4f } \\t ' f 'duration= { self . duration [ - 1 ] } ' ))","title":"Progress"},{"location":"trainer/progress/#cltrier_nlp.trainer.progress.Progress","text":"Bases: BaseModel Source code in cltrier_nlp/trainer/progress.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class Progress ( pydantic . BaseModel ): epoch : typing . List [ int ] = [] duration : typing . List [ datetime . timedelta ] = [] loss_train : typing . List [ float ] = [] loss_test : typing . List [ float ] = [] f1_train : typing . List [ float ] = [] f1_test : typing . List [ float ] = [] metric_train : typing . List [ dict ] = [] metric_test : typing . List [ dict ] = [] def append_record ( self , epoch : int , duration : datetime . timedelta , train_results : typing . Tuple [ float , float , dict ], test_results : typing . Tuple [ float , float , dict ] ): \"\"\" Append a record of the epoch, duration, train results, and test results to their respective lists. Args: epoch (int): The epoch number. duration (datetime.timedelta): The duration of the training. train_results (typing.Tuple[float, float, dict]): The results of the training. test_results (typing.Tuple[float, float, dict]): The results of the testing. \"\"\" self . epoch . append ( epoch ) self . duration . append ( duration ) self . loss_train . append ( train_results [ 0 ]) self . loss_test . append ( test_results [ 0 ]) self . f1_train . append ( train_results [ 1 ]) self . f1_test . append ( test_results [ 1 ]) self . metric_train . append ( train_results [ 2 ]) self . metric_test . append ( test_results [ 2 ]) @pydantic . computed_field # type: ignore[misc] @property def max_record_id ( self ) -> int : \"\"\" Returns the maximum record id as an integer. \"\"\" if not self . f1_test : return - 1 return self . f1_test . index ( max ( self . f1_test )) @pydantic . computed_field # type: ignore[misc] @property def last_is_best ( self ) -> bool : \"\"\" Returns a boolean indicating whether the last element in f1_test is the maximum. \"\"\" if not self . f1_test : return False return self . f1_test [ - 1 ] == max ( self . f1_test ) def log ( self ) -> None : \"\"\" Logs the training and testing metrics, including loss and F1 scores, along with the epoch number and duration. \"\"\" logging . info (( f '[@ { self . epoch [ - 1 ] : 03 } ]: \\t ' f 'loss_train= { self . loss_train [ - 1 ] : 2.4f } \\t ' f 'loss_test= { self . loss_test [ - 1 ] : 2.4f } \\t ' f 'f1_train= { self . f1_train [ - 1 ] : 2.4f } \\t ' f 'f1_test= { self . f1_test [ - 1 ] : 2.4f } \\t ' f 'duration= { self . duration [ - 1 ] } ' )) def export ( self , path : str ) -> None : \"\"\" Exports the record dump to a CSV file at the specified path. Args: path (str): The file path to export the CSV. \"\"\" ( pd . DataFrame . from_records ( self . model_dump ( exclude = { 'metric_train' , 'metric_test' } ), index = [ 'epoch' ]) . to_csv ( f ' { path } .csv' ) )","title":"Progress"},{"location":"trainer/progress/#cltrier_nlp.trainer.progress.Progress.last_is_best","text":"Returns a boolean indicating whether the last element in f1_test is the maximum.","title":"last_is_best"},{"location":"trainer/progress/#cltrier_nlp.trainer.progress.Progress.max_record_id","text":"Returns the maximum record id as an integer.","title":"max_record_id"},{"location":"trainer/progress/#cltrier_nlp.trainer.progress.Progress.append_record","text":"Append a record of the epoch, duration, train results, and test results to their respective lists. Parameters: epoch ( int ) \u2013 The epoch number. duration ( timedelta ) \u2013 The duration of the training. train_results ( Tuple [ float , float , dict ] ) \u2013 The results of the training. test_results ( Tuple [ float , float , dict ] ) \u2013 The results of the testing. Source code in cltrier_nlp/trainer/progress.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def append_record ( self , epoch : int , duration : datetime . timedelta , train_results : typing . Tuple [ float , float , dict ], test_results : typing . Tuple [ float , float , dict ] ): \"\"\" Append a record of the epoch, duration, train results, and test results to their respective lists. Args: epoch (int): The epoch number. duration (datetime.timedelta): The duration of the training. train_results (typing.Tuple[float, float, dict]): The results of the training. test_results (typing.Tuple[float, float, dict]): The results of the testing. \"\"\" self . epoch . append ( epoch ) self . duration . append ( duration ) self . loss_train . append ( train_results [ 0 ]) self . loss_test . append ( test_results [ 0 ]) self . f1_train . append ( train_results [ 1 ]) self . f1_test . append ( test_results [ 1 ]) self . metric_train . append ( train_results [ 2 ]) self . metric_test . append ( test_results [ 2 ])","title":"append_record"},{"location":"trainer/progress/#cltrier_nlp.trainer.progress.Progress.export","text":"Exports the record dump to a CSV file at the specified path. Parameters: path ( str ) \u2013 The file path to export the CSV. Source code in cltrier_nlp/trainer/progress.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def export ( self , path : str ) -> None : \"\"\" Exports the record dump to a CSV file at the specified path. Args: path (str): The file path to export the CSV. \"\"\" ( pd . DataFrame . from_records ( self . model_dump ( exclude = { 'metric_train' , 'metric_test' } ), index = [ 'epoch' ]) . to_csv ( f ' { path } .csv' ) )","title":"export"},{"location":"trainer/progress/#cltrier_nlp.trainer.progress.Progress.log","text":"Logs the training and testing metrics, including loss and F1 scores, along with the epoch number and duration. Source code in cltrier_nlp/trainer/progress.py 73 74 75 76 77 78 79 80 81 82 83 84 def log ( self ) -> None : \"\"\" Logs the training and testing metrics, including loss and F1 scores, along with the epoch number and duration. \"\"\" logging . info (( f '[@ { self . epoch [ - 1 ] : 03 } ]: \\t ' f 'loss_train= { self . loss_train [ - 1 ] : 2.4f } \\t ' f 'loss_test= { self . loss_test [ - 1 ] : 2.4f } \\t ' f 'f1_train= { self . f1_train [ - 1 ] : 2.4f } \\t ' f 'f1_test= { self . f1_test [ - 1 ] : 2.4f } \\t ' f 'duration= { self . duration [ - 1 ] } ' ))","title":"log"},{"location":"utility/","text":"Map Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) __init__ ( keys ) Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} __len__ () Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) get_ids ( keys ) Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] get_keys ( ids ) Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"Utility"},{"location":"utility/#cltrier_nlp.utility.Map","text":"Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"Map"},{"location":"utility/#cltrier_nlp.utility.Map.__init__","text":"Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()}","title":"__init__"},{"location":"utility/#cltrier_nlp.utility.Map.__len__","text":"Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"__len__"},{"location":"utility/#cltrier_nlp.utility.Map.get_ids","text":"Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ]","title":"get_ids"},{"location":"utility/#cltrier_nlp.utility.Map.get_keys","text":"Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"get_keys"},{"location":"utility/map/","text":"Map Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) __init__ ( keys ) Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} __len__ () Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) get_ids ( keys ) Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] get_keys ( ids ) Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"Map"},{"location":"utility/map/#cltrier_nlp.utility.map.Map","text":"Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"Map"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.__init__","text":"Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()}","title":"__init__"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.__len__","text":"Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"__len__"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.get_ids","text":"Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ]","title":"get_ids"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.get_keys","text":"Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"get_keys"},{"location":"utility/types/","text":"","title":"Types"}]}