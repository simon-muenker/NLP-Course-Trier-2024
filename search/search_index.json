{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"corpus sentence encoder batch pooler functional neural text utility map","title":"Home"},{"location":"corpus/","text":"Corpus Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Corpus ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str sentences : typing . List [ Sentence ] = [] args : CorpusArgs = CorpusArgs () @functional . timeit def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] @pydantic . computed_field # type: ignore[misc] @property def tokens ( self ) -> typing . List [ str ]: \"\"\" \"\"\" return [ tok for sent in self . sentences for tok in sent . tokens ] @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 5 ) def count_languages ( self ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) def count_tokens ( self ) -> collections . Counter : \"\"\" \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ( self . generate_ngrams ( n )) def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), ) def generate_ngrams ( self , n : int ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" \"\"\" return cls ( raw = open ( path ) . read ()) def to_df ( self ) -> pandas . DataFrame : \"\"\" \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . sentences ) bigrams : typing . List [ typing . Tuple [ str , ... ]] property pentagram : typing . List [ typing . Tuple [ str , ... ]] property tetragram : typing . List [ typing . Tuple [ str , ... ]] property tokens : typing . List [ str ] property trigrams : typing . List [ typing . Tuple [ str , ... ]] property __len__ () Source code in cltrier_nlp/corpus/__init__.py 143 144 145 146 147 def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . sentences ) count_languages () Source code in cltrier_nlp/corpus/__init__.py 84 85 86 87 88 def count_languages ( self ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) count_ngrams ( n ) Source code in cltrier_nlp/corpus/__init__.py 103 104 105 106 107 def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ( self . generate_ngrams ( n )) count_tokens () Source code in cltrier_nlp/corpus/__init__.py 90 91 92 93 94 95 96 97 98 99 100 101 def count_tokens ( self ) -> collections . Counter : \"\"\" \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) create_subset_by_language ( language ) Source code in cltrier_nlp/corpus/__init__.py 109 110 111 112 113 114 115 116 117 118 def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), ) from_txt ( path ) classmethod Source code in cltrier_nlp/corpus/__init__.py 128 129 130 131 132 133 @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" \"\"\" return cls ( raw = open ( path ) . read ()) generate_ngrams ( n ) Source code in cltrier_nlp/corpus/__init__.py 120 121 122 123 124 125 126 def generate_ngrams ( self , n : int ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] model_post_init ( __context ) Source code in cltrier_nlp/corpus/__init__.py 34 35 36 37 38 39 40 41 42 @functional . timeit def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] to_df () Source code in cltrier_nlp/corpus/__init__.py 135 136 137 138 139 140 141 def to_df ( self ) -> pandas . DataFrame : \"\"\" \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) CorpusArgs Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 17 18 19 20 21 22 23 class CorpusArgs ( pydantic . BaseModel ): \"\"\" \"\"\" token_count_exclude : typing . List [ str ] = pydantic . Field ( default_factory = lambda : [ \"\u2019\" , \"\u201c\" , \"\u201d\" , * string . punctuation ] ) Sentence Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = UNK_LANG tokens : typing . List [ str ] = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens ) bigrams : typing . List [ typing . Tuple [ str , ... ]] property pentagram : typing . List [ typing . Tuple [ str , ... ]] property tetragram : typing . List [ typing . Tuple [ str , ... ]] property trigrams : typing . List [ typing . Tuple [ str , ... ]] property __len__ () Source code in cltrier_nlp/corpus/sentence.py 70 71 72 73 74 def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens ) model_post_init ( __context ) Source code in cltrier_nlp/corpus/sentence.py 20 21 22 23 24 25 26 27 28 29 30 def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) to_row () Source code in cltrier_nlp/corpus/sentence.py 64 65 66 67 68 def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ())","title":"Corpus"},{"location":"corpus/#cltrier_nlp.corpus.Corpus","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Corpus ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str sentences : typing . List [ Sentence ] = [] args : CorpusArgs = CorpusArgs () @functional . timeit def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] @pydantic . computed_field # type: ignore[misc] @property def tokens ( self ) -> typing . List [ str ]: \"\"\" \"\"\" return [ tok for sent in self . sentences for tok in sent . tokens ] @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return self . generate_ngrams ( 5 ) def count_languages ( self ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) def count_tokens ( self ) -> collections . Counter : \"\"\" \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ( self . generate_ngrams ( n )) def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), ) def generate_ngrams ( self , n : int ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" \"\"\" return cls ( raw = open ( path ) . read ()) def to_df ( self ) -> pandas . DataFrame : \"\"\" \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . sentences )","title":"Corpus"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.bigrams","text":"","title":"bigrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.pentagram","text":"","title":"pentagram"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.tetragram","text":"","title":"tetragram"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.tokens","text":"","title":"tokens"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.trigrams","text":"","title":"trigrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.__len__","text":"Source code in cltrier_nlp/corpus/__init__.py 143 144 145 146 147 def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . sentences )","title":"__len__"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_languages","text":"Source code in cltrier_nlp/corpus/__init__.py 84 85 86 87 88 def count_languages ( self ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ])","title":"count_languages"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_ngrams","text":"Source code in cltrier_nlp/corpus/__init__.py 103 104 105 106 107 def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" \"\"\" return collections . Counter ( self . generate_ngrams ( n ))","title":"count_ngrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_tokens","text":"Source code in cltrier_nlp/corpus/__init__.py 90 91 92 93 94 95 96 97 98 99 100 101 def count_tokens ( self ) -> collections . Counter : \"\"\" \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ])","title":"count_tokens"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.create_subset_by_language","text":"Source code in cltrier_nlp/corpus/__init__.py 109 110 111 112 113 114 115 116 117 118 def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), )","title":"create_subset_by_language"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.from_txt","text":"Source code in cltrier_nlp/corpus/__init__.py 128 129 130 131 132 133 @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" \"\"\" return cls ( raw = open ( path ) . read ())","title":"from_txt"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.generate_ngrams","text":"Source code in cltrier_nlp/corpus/__init__.py 120 121 122 123 124 125 126 def generate_ngrams ( self , n : int ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ]","title":"generate_ngrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.model_post_init","text":"Source code in cltrier_nlp/corpus/__init__.py 34 35 36 37 38 39 40 41 42 @functional . timeit def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ]","title":"model_post_init"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.to_df","text":"Source code in cltrier_nlp/corpus/__init__.py 135 136 137 138 139 140 141 def to_df ( self ) -> pandas . DataFrame : \"\"\" \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], )","title":"to_df"},{"location":"corpus/#cltrier_nlp.corpus.CorpusArgs","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 17 18 19 20 21 22 23 class CorpusArgs ( pydantic . BaseModel ): \"\"\" \"\"\" token_count_exclude : typing . List [ str ] = pydantic . Field ( default_factory = lambda : [ \"\u2019\" , \"\u201c\" , \"\u201d\" , * string . punctuation ] )","title":"CorpusArgs"},{"location":"corpus/#cltrier_nlp.corpus.Sentence","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = UNK_LANG tokens : typing . List [ str ] = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens )","title":"Sentence"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.bigrams","text":"","title":"bigrams"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.pentagram","text":"","title":"pentagram"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.tetragram","text":"","title":"tetragram"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.trigrams","text":"","title":"trigrams"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.__len__","text":"Source code in cltrier_nlp/corpus/sentence.py 70 71 72 73 74 def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens )","title":"__len__"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.model_post_init","text":"Source code in cltrier_nlp/corpus/sentence.py 20 21 22 23 24 25 26 27 28 29 30 def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw )","title":"model_post_init"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.to_row","text":"Source code in cltrier_nlp/corpus/sentence.py 64 65 66 67 68 def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ())","title":"to_row"},{"location":"corpus/sentence/","text":"Sentence Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = UNK_LANG tokens : typing . List [ str ] = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens ) bigrams : typing . List [ typing . Tuple [ str , ... ]] property pentagram : typing . List [ typing . Tuple [ str , ... ]] property tetragram : typing . List [ typing . Tuple [ str , ... ]] property trigrams : typing . List [ typing . Tuple [ str , ... ]] property __len__ () Source code in cltrier_nlp/corpus/sentence.py 70 71 72 73 74 def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens ) model_post_init ( __context ) Source code in cltrier_nlp/corpus/sentence.py 20 21 22 23 24 25 26 27 28 29 30 def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) to_row () Source code in cltrier_nlp/corpus/sentence.py 64 65 66 67 68 def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ())","title":"Sentence"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = UNK_LANG tokens : typing . List [ str ] = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens )","title":"Sentence"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.bigrams","text":"","title":"bigrams"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.pentagram","text":"","title":"pentagram"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.tetragram","text":"","title":"tetragram"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.trigrams","text":"","title":"trigrams"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.__len__","text":"Source code in cltrier_nlp/corpus/sentence.py 70 71 72 73 74 def __len__ ( self ) -> int : \"\"\" \"\"\" return len ( self . tokens )","title":"__len__"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.model_post_init","text":"Source code in cltrier_nlp/corpus/sentence.py 20 21 22 23 24 25 26 27 28 29 30 def model_post_init ( self , __context ) -> None : \"\"\" \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw )","title":"model_post_init"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.to_row","text":"Source code in cltrier_nlp/corpus/sentence.py 64 65 66 67 68 def to_row ( self ) -> pandas . Series : \"\"\" \"\"\" return pandas . Series ( self . model_dump ())","title":"to_row"},{"location":"encoder/","text":"EncodedBatch Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class EncodedBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . List [ torch . Tensor ] token : typing . List [ typing . List [ str ]] input_ids : typing . List [ typing . List [ int ]] token_type_ids : typing . List [ typing . List [ int ]] attention_mask : typing . List [ typing . List [ int ]] offset_mapping : typing . List [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )] Encoder Bases: Module Source code in cltrier_nlp/encoder/__init__.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class Encoder ( torch . nn . Module ): \"\"\" \"\"\" @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\" \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) def __call__ ( self , batch : typing . List [ str ], unpad : bool = True ) -> EncodedBatch : \"\"\" \"\"\" encoding , token = self . tokenize ( batch ) embeds : torch . Tensor = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ) return EncodedBatch ( ** { \"embeds\" : embeds , \"token\" : token , \"unpad\" : unpad , } | encoding ) def tokenize ( self , batch : typing . List [ str ], padding : bool = True ) -> typing . Tuple [ typing . Dict , typing . List [ typing . List [ str ]]]: \"\"\" \"\"\" return ( encoding := self . tokenizer ( batch , padding = padding , ** self . args . tokenizer ), [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], ) def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) @property def dim ( self ) -> int : \"\"\" \"\"\" return self . model . config . to_dict ()[ \"hidden_size\" ] def __len__ ( self ) -> int : \"\"\" \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] def __repr__ ( self ) -> str : \"\"\" \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" ) dim : int property __call__ ( batch , unpad = True ) Source code in cltrier_nlp/encoder/__init__.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __call__ ( self , batch : typing . List [ str ], unpad : bool = True ) -> EncodedBatch : \"\"\" \"\"\" encoding , token = self . tokenize ( batch ) embeds : torch . Tensor = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ) return EncodedBatch ( ** { \"embeds\" : embeds , \"token\" : token , \"unpad\" : unpad , } | encoding ) __init__ ( args = EncoderArgs ()) Source code in cltrier_nlp/encoder/__init__.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\" \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) __len__ () Source code in cltrier_nlp/encoder/__init__.py 116 117 118 119 120 def __len__ ( self ) -> int : \"\"\" \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] __repr__ () Source code in cltrier_nlp/encoder/__init__.py 122 123 124 125 126 127 128 129 def __repr__ ( self ) -> str : \"\"\" \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" ) forward ( ids , masks ) Source code in cltrier_nlp/encoder/__init__.py 85 86 87 88 89 90 91 92 93 94 95 def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) ids_to_sent ( ids ) Source code in cltrier_nlp/encoder/__init__.py 103 104 105 106 107 def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) ids_to_tokens ( ids ) Source code in cltrier_nlp/encoder/__init__.py 97 98 99 100 101 def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) tokenize ( batch , padding = True ) Source code in cltrier_nlp/encoder/__init__.py 74 75 76 77 78 79 80 81 82 83 def tokenize ( self , batch : typing . List [ str ], padding : bool = True ) -> typing . Tuple [ typing . Dict , typing . List [ typing . List [ str ]]]: \"\"\" \"\"\" return ( encoding := self . tokenizer ( batch , padding = padding , ** self . args . tokenizer ), [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], ) EncoderArgs Bases: BaseModel Source code in cltrier_nlp/encoder/__init__.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class EncoderArgs ( pydantic . BaseModel ): \"\"\" \"\"\" model : str = \"prajjwal1/bert-tiny\" layers : typing . List [ int ] = [ - 1 ] device : str = functional . neural . get_device () tokenizer : typing . Dict [ str , str | int ] = dict ( max_length = 512 , truncation = True , return_offsets_mapping = True , ) EncoderPooler Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncodedBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ]) __call__ ( encodes , extract_spans = None , form = EncoderPoolerArgs () . types ) Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"Encoder"},{"location":"encoder/#cltrier_nlp.encoder.EncodedBatch","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class EncodedBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . List [ torch . Tensor ] token : typing . List [ typing . List [ str ]] input_ids : typing . List [ typing . List [ int ]] token_type_ids : typing . List [ typing . List [ int ]] attention_mask : typing . List [ typing . List [ int ]] offset_mapping : typing . List [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"EncodedBatch"},{"location":"encoder/#cltrier_nlp.encoder.Encoder","text":"Bases: Module Source code in cltrier_nlp/encoder/__init__.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class Encoder ( torch . nn . Module ): \"\"\" \"\"\" @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\" \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) def __call__ ( self , batch : typing . List [ str ], unpad : bool = True ) -> EncodedBatch : \"\"\" \"\"\" encoding , token = self . tokenize ( batch ) embeds : torch . Tensor = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ) return EncodedBatch ( ** { \"embeds\" : embeds , \"token\" : token , \"unpad\" : unpad , } | encoding ) def tokenize ( self , batch : typing . List [ str ], padding : bool = True ) -> typing . Tuple [ typing . Dict , typing . List [ typing . List [ str ]]]: \"\"\" \"\"\" return ( encoding := self . tokenizer ( batch , padding = padding , ** self . args . tokenizer ), [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], ) def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) @property def dim ( self ) -> int : \"\"\" \"\"\" return self . model . config . to_dict ()[ \"hidden_size\" ] def __len__ ( self ) -> int : \"\"\" \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] def __repr__ ( self ) -> str : \"\"\" \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" )","title":"Encoder"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.dim","text":"","title":"dim"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__call__","text":"Source code in cltrier_nlp/encoder/__init__.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __call__ ( self , batch : typing . List [ str ], unpad : bool = True ) -> EncodedBatch : \"\"\" \"\"\" encoding , token = self . tokenize ( batch ) embeds : torch . Tensor = self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ) return EncodedBatch ( ** { \"embeds\" : embeds , \"token\" : token , \"unpad\" : unpad , } | encoding )","title":"__call__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__init__","text":"Source code in cltrier_nlp/encoder/__init__.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\" \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self )","title":"__init__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__len__","text":"Source code in cltrier_nlp/encoder/__init__.py 116 117 118 119 120 def __len__ ( self ) -> int : \"\"\" \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ]","title":"__len__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__repr__","text":"Source code in cltrier_nlp/encoder/__init__.py 122 123 124 125 126 127 128 129 def __repr__ ( self ) -> str : \"\"\" \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" )","title":"__repr__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.forward","text":"Source code in cltrier_nlp/encoder/__init__.py 85 86 87 88 89 90 91 92 93 94 95 def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () )","title":"forward"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.ids_to_sent","text":"Source code in cltrier_nlp/encoder/__init__.py 103 104 105 106 107 def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True )","title":"ids_to_sent"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.ids_to_tokens","text":"Source code in cltrier_nlp/encoder/__init__.py 97 98 99 100 101 def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids )","title":"ids_to_tokens"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.tokenize","text":"Source code in cltrier_nlp/encoder/__init__.py 74 75 76 77 78 79 80 81 82 83 def tokenize ( self , batch : typing . List [ str ], padding : bool = True ) -> typing . Tuple [ typing . Dict , typing . List [ typing . List [ str ]]]: \"\"\" \"\"\" return ( encoding := self . tokenizer ( batch , padding = padding , ** self . args . tokenizer ), [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], )","title":"tokenize"},{"location":"encoder/#cltrier_nlp.encoder.EncoderArgs","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/__init__.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class EncoderArgs ( pydantic . BaseModel ): \"\"\" \"\"\" model : str = \"prajjwal1/bert-tiny\" layers : typing . List [ int ] = [ - 1 ] device : str = functional . neural . get_device () tokenizer : typing . Dict [ str , str | int ] = dict ( max_length = 512 , truncation = True , return_offsets_mapping = True , )","title":"EncoderArgs"},{"location":"encoder/#cltrier_nlp.encoder.EncoderPooler","text":"Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncodedBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ])","title":"EncoderPooler"},{"location":"encoder/#cltrier_nlp.encoder.EncoderPooler.__call__","text":"Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"__call__"},{"location":"encoder/batch/","text":"EncodedBatch Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class EncodedBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . List [ torch . Tensor ] token : typing . List [ typing . List [ str ]] input_ids : typing . List [ typing . List [ int ]] token_type_ids : typing . List [ typing . List [ int ]] attention_mask : typing . List [ typing . List [ int ]] offset_mapping : typing . List [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"Batch"},{"location":"encoder/batch/#cltrier_nlp.encoder.batch.EncodedBatch","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class EncodedBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : typing . List [ torch . Tensor ] token : typing . List [ typing . List [ str ]] input_ids : typing . List [ typing . List [ int ]] token_type_ids : typing . List [ typing . List [ int ]] attention_mask : typing . List [ typing . List [ int ]] offset_mapping : typing . List [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"EncodedBatch"},{"location":"encoder/pooler/","text":"EncoderPooler Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncodedBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ]) __call__ ( encodes , extract_spans = None , form = EncoderPoolerArgs () . types ) Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] EncoderPoolerArgs Bases: BaseModel Source code in cltrier_nlp/encoder/pooler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EncoderPoolerArgs ( pydantic . BaseModel ): \"\"\" \"\"\" fns : typing . Dict [ str , typing . Callable ] = { # sentence based \"sent_cls\" : lambda x : x [ 0 ], \"sent_mean\" : lambda x : torch . mean ( x [ 1 : - 1 ], dim = 0 ), # word based, positional extraction \"subword_first\" : lambda x : x [ 0 ], \"subword_last\" : lambda x : x [ - 1 ], # word based, arithmetic extraction \"subword_mean\" : lambda x : torch . mean ( x , dim = 0 ), \"subword_min\" : lambda x : torch . min ( x , dim = 0 )[ 0 ], \"subword_max\" : lambda x : torch . max ( x , dim = 0 )[ 0 ], } types : typing . Literal [ \"sent_cls\" , \"sent_mean\" , \"subword_first\" , \"subword_last\" , \"subword_mean\" , \"subword_min\" , \"subword_max\" , ] = \"sent_cls\"","title":"Pooler"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPooler","text":"Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncodedBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ])","title":"EncoderPooler"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPooler.__call__","text":"Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncodedBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"__call__"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPoolerArgs","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/pooler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EncoderPoolerArgs ( pydantic . BaseModel ): \"\"\" \"\"\" fns : typing . Dict [ str , typing . Callable ] = { # sentence based \"sent_cls\" : lambda x : x [ 0 ], \"sent_mean\" : lambda x : torch . mean ( x [ 1 : - 1 ], dim = 0 ), # word based, positional extraction \"subword_first\" : lambda x : x [ 0 ], \"subword_last\" : lambda x : x [ - 1 ], # word based, arithmetic extraction \"subword_mean\" : lambda x : torch . mean ( x , dim = 0 ), \"subword_min\" : lambda x : torch . min ( x , dim = 0 )[ 0 ], \"subword_max\" : lambda x : torch . max ( x , dim = 0 )[ 0 ], } types : typing . Literal [ \"sent_cls\" , \"sent_mean\" , \"subword_first\" , \"subword_last\" , \"subword_mean\" , \"subword_min\" , \"subword_max\" , ] = \"sent_cls\"","title":"EncoderPoolerArgs"},{"location":"functional/","text":"timeit ( func ) Source code in cltrier_nlp/functional/__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def timeit ( func : typing . Callable ): \"\"\" \"\"\" @functools . wraps ( func ) def wrap ( * args , ** kwargs ) -> typing . Any : start = time . time () result = func ( * args , ** kwargs ) logging . info ( f \"> f( { func . __name__ } ) took: { time . time () - start : 2.4f } sec\" ) return result return wrap","title":"Functional"},{"location":"functional/#cltrier_nlp.functional.timeit","text":"Source code in cltrier_nlp/functional/__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def timeit ( func : typing . Callable ): \"\"\" \"\"\" @functools . wraps ( func ) def wrap ( * args , ** kwargs ) -> typing . Any : start = time . time () result = func ( * args , ** kwargs ) logging . info ( f \"> f( { func . __name__ } ) took: { time . time () - start : 2.4f } sec\" ) return result return wrap","title":"timeit"},{"location":"functional/neural/","text":"calculate_model_memory_usage ( model ) Source code in cltrier_nlp/functional/neural.py 11 12 13 14 15 16 17 18 19 20 21 22 def calculate_model_memory_usage ( model : torch . nn . Module ) -> str : \"\"\" \"\"\" usage_in_byte : int = sum ( [ sum ([ param . nelement () * param . element_size () for param in model . parameters ()]), sum ([ buf . nelement () * buf . element_size () for buf in model . buffers ()]), ] ) return f \" { usage_in_byte / ( 1024.0 * 1024.0 ) : 2.4f } MB\" get_device () Source code in cltrier_nlp/functional/neural.py 4 5 6 7 8 def get_device () -> str : \"\"\" \"\"\" return \"cuda\" if torch . cuda . is_available () else \"cpu\"","title":"Neural"},{"location":"functional/neural/#cltrier_nlp.functional.neural.calculate_model_memory_usage","text":"Source code in cltrier_nlp/functional/neural.py 11 12 13 14 15 16 17 18 19 20 21 22 def calculate_model_memory_usage ( model : torch . nn . Module ) -> str : \"\"\" \"\"\" usage_in_byte : int = sum ( [ sum ([ param . nelement () * param . element_size () for param in model . parameters ()]), sum ([ buf . nelement () * buf . element_size () for buf in model . buffers ()]), ] ) return f \" { usage_in_byte / ( 1024.0 * 1024.0 ) : 2.4f } MB\"","title":"calculate_model_memory_usage"},{"location":"functional/neural/#cltrier_nlp.functional.neural.get_device","text":"Source code in cltrier_nlp/functional/neural.py 4 5 6 7 8 def get_device () -> str : \"\"\" \"\"\" return \"cuda\" if torch . cuda . is_available () else \"cpu\"","title":"get_device"},{"location":"functional/text/","text":"detect_language ( content ) Source code in cltrier_nlp/functional/text.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def detect_language ( content : str ) -> str : \"\"\" \"\"\" # Ignore langcodes dependent language data warning # DeprecationWarning: pkg_resources is deprecated as an API. with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) try : return langcodes . Language . get ( langdetect . detect ( content )) . display_name () . lower () except langdetect . lang_detect_exception . LangDetectException : return \"unknown\" load_stopwords ( languages ) Source code in cltrier_nlp/functional/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def load_stopwords ( languages : typing . List [ str ]) -> typing . List [ str ]: \"\"\" \"\"\" return list ( set () . union ( * [ nltk . corpus . stopwords . words ( lang ) for lang in languages if lang in nltk . corpus . stopwords . fileids () ] ) ) ngrams ( tokens , n ) Source code in cltrier_nlp/functional/text.py 42 43 44 45 46 def ngrams ( tokens : typing . List [ str ], n : int ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return [ tuple ( tokens [ i : i + n ]) for i in range ( len ( tokens ) - n + 1 )] sentenize ( text ) Source code in cltrier_nlp/functional/text.py 24 25 26 27 28 def sentenize ( text : str ) -> typing . List [ str ]: \"\"\" \"\"\" return nltk . tokenize . sent_tokenize ( text , language = detect_language ( text )) tokenize ( text ) Source code in cltrier_nlp/functional/text.py 31 32 33 34 35 36 37 38 39 def tokenize ( text : str ) -> typing . List [ str ]: \"\"\" \"\"\" try : return nltk . tokenize . word_tokenize ( text . lower (), language = detect_language ( text )) except LookupError : return nltk . tokenize . word_tokenize ( text . lower ())","title":"Text"},{"location":"functional/text/#cltrier_nlp.functional.text.detect_language","text":"Source code in cltrier_nlp/functional/text.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def detect_language ( content : str ) -> str : \"\"\" \"\"\" # Ignore langcodes dependent language data warning # DeprecationWarning: pkg_resources is deprecated as an API. with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) try : return langcodes . Language . get ( langdetect . detect ( content )) . display_name () . lower () except langdetect . lang_detect_exception . LangDetectException : return \"unknown\"","title":"detect_language"},{"location":"functional/text/#cltrier_nlp.functional.text.load_stopwords","text":"Source code in cltrier_nlp/functional/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def load_stopwords ( languages : typing . List [ str ]) -> typing . List [ str ]: \"\"\" \"\"\" return list ( set () . union ( * [ nltk . corpus . stopwords . words ( lang ) for lang in languages if lang in nltk . corpus . stopwords . fileids () ] ) )","title":"load_stopwords"},{"location":"functional/text/#cltrier_nlp.functional.text.ngrams","text":"Source code in cltrier_nlp/functional/text.py 42 43 44 45 46 def ngrams ( tokens : typing . List [ str ], n : int ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" \"\"\" return [ tuple ( tokens [ i : i + n ]) for i in range ( len ( tokens ) - n + 1 )]","title":"ngrams"},{"location":"functional/text/#cltrier_nlp.functional.text.sentenize","text":"Source code in cltrier_nlp/functional/text.py 24 25 26 27 28 def sentenize ( text : str ) -> typing . List [ str ]: \"\"\" \"\"\" return nltk . tokenize . sent_tokenize ( text , language = detect_language ( text ))","title":"sentenize"},{"location":"functional/text/#cltrier_nlp.functional.text.tokenize","text":"Source code in cltrier_nlp/functional/text.py 31 32 33 34 35 36 37 38 39 def tokenize ( text : str ) -> typing . List [ str ]: \"\"\" \"\"\" try : return nltk . tokenize . word_tokenize ( text . lower (), language = detect_language ( text )) except LookupError : return nltk . tokenize . word_tokenize ( text . lower ())","title":"tokenize"},{"location":"utility/","text":"Map Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) __init__ ( keys ) Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} __len__ () Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) get_ids ( keys ) Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] get_keys ( ids ) Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"Utility"},{"location":"utility/#cltrier_nlp.utility.Map","text":"Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"Map"},{"location":"utility/#cltrier_nlp.utility.Map.__init__","text":"Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()}","title":"__init__"},{"location":"utility/#cltrier_nlp.utility.Map.__len__","text":"Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"__len__"},{"location":"utility/#cltrier_nlp.utility.Map.get_ids","text":"Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ]","title":"get_ids"},{"location":"utility/#cltrier_nlp.utility.Map.get_keys","text":"Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"get_keys"},{"location":"utility/map/","text":"Map Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) __init__ ( keys ) Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} __len__ () Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) get_ids ( keys ) Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] get_keys ( ids ) Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"Map"},{"location":"utility/map/#cltrier_nlp.utility.map.Map","text":"Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"Map"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.__init__","text":"Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()}","title":"__init__"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.__len__","text":"Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"__len__"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.get_ids","text":"Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ]","title":"get_ids"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.get_keys","text":"Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"get_keys"}]}