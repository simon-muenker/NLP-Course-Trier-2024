{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started Usage Install pip install cltrier_nlp Development Install The project is managed by Poetry, a dependency management and packaging library. Please set up a local version according to the official installation guidelines . When finished, install the local repository as follows: # install package dependencies poetry install # add pre-commit to git hooks poetry run pre-commit install Tests poetry run pytest Linting poetry run pre-commit run --all-files Sitemap corpus sentence encoder batch pooler functional neural text utility map types","title":"Getting started"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#usage","text":"","title":"Usage"},{"location":"#install","text":"pip install cltrier_nlp","title":"Install"},{"location":"#development","text":"","title":"Development"},{"location":"#install_1","text":"The project is managed by Poetry, a dependency management and packaging library. Please set up a local version according to the official installation guidelines . When finished, install the local repository as follows: # install package dependencies poetry install # add pre-commit to git hooks poetry run pre-commit install","title":"Install"},{"location":"#tests","text":"poetry run pytest","title":"Tests"},{"location":"#linting","text":"poetry run pre-commit run --all-files","title":"Linting"},{"location":"#sitemap","text":"corpus sentence encoder batch pooler functional neural text utility map types","title":"Sitemap"},{"location":"corpus/","text":"Corpus Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Corpus ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str sentences : typing . List [ Sentence ] = [] args : CorpusArgs = CorpusArgs () @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] @pydantic . computed_field # type: ignore[misc] @property def tokens ( self ) -> utility . types . Tokens : \"\"\" Returns: \"\"\" return [ tok for sent in self . sentences for tok in sent . tokens ] @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 5 ) def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n )) def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), ) def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ()) def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences ) bigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: pentagram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: tetragram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: tokens : utility . types . Tokens property Returns: trigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: __len__ () Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/__init__.py 185 186 187 188 189 190 191 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences ) count_languages () Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 100 101 102 103 104 105 106 def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) count_ngrams ( n ) Parameters: n ( int ) \u2013 Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 123 124 125 126 127 128 129 130 131 132 def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n )) count_tokens () Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) create_subset_by_language ( language ) Parameters: language ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), ) from_txt ( path ) classmethod Parameters: path ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 163 164 165 166 167 168 169 170 171 172 173 @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ()) generate_ngrams ( n ) Parameters: n ( int ) \u2013 Returns: NGrams \u2013 utility.types.NGrams: Source code in cltrier_nlp/corpus/__init__.py 150 151 152 153 154 155 156 157 158 159 160 161 def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] model_post_init ( __context ) Parameters: __context ( Any ) \u2013 ?? Returns: Source code in cltrier_nlp/corpus/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] to_df () Returns: DataFrame \u2013 pandas.DataFrame: Source code in cltrier_nlp/corpus/__init__.py 175 176 177 178 179 180 181 182 183 def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) CorpusArgs Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 18 19 20 21 22 23 24 class CorpusArgs ( pydantic . BaseModel ): \"\"\" \"\"\" token_count_exclude : utility . types . Tokens = pydantic . Field ( default_factory = lambda : [ \"\u2019\" , \"\u201c\" , \"\u201d\" , * string . punctuation ] ) Sentence Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) bigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: pentagram : typing . List [ typing . Tuple [ str , ... ]] property Returns: tetragram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: trigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: __len__ () Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) model_post_init ( __context ) Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) to_row () Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"Corpus"},{"location":"corpus/#cltrier_nlp.corpus.Corpus","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Corpus ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str sentences : typing . List [ Sentence ] = [] args : CorpusArgs = CorpusArgs () @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ] @pydantic . computed_field # type: ignore[misc] @property def tokens ( self ) -> utility . types . Tokens : \"\"\" Returns: \"\"\" return [ tok for sent in self . sentences for tok in sent . tokens ] @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return self . generate_ngrams ( 5 ) def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ]) def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ]) def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n )) def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), ) def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ] @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ()) def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], ) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences )","title":"Corpus"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.bigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"bigrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.pentagram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"pentagram"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.tetragram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"tetragram"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.tokens","text":"Returns:","title":"tokens"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.trigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"trigrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.__len__","text":"Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/__init__.py 185 186 187 188 189 190 191 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . sentences )","title":"__len__"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_languages","text":"Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 100 101 102 103 104 105 106 def count_languages ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" return collections . Counter ([ sent . language for sent in self . sentences ])","title":"count_languages"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_ngrams","text":"Parameters: n ( int ) \u2013 Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 123 124 125 126 127 128 129 130 131 132 def count_ngrams ( self , n : int ) -> collections . Counter : \"\"\" Args: n (int): Returns: collections.Counter: \"\"\" return collections . Counter ( self . generate_ngrams ( n ))","title":"count_ngrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.count_tokens","text":"Returns: Counter \u2013 collections.Counter: Source code in cltrier_nlp/corpus/__init__.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def count_tokens ( self ) -> collections . Counter : \"\"\" Returns: collections.Counter: \"\"\" filter_words = [ * functional . text . load_stopwords ( list ( set ([ sent . language for sent in self . sentences ])) ), * self . args . token_count_exclude , ] return collections . Counter ([ tok for tok in self . tokens if tok not in filter_words ])","title":"count_tokens"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.create_subset_by_language","text":"Parameters: language ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def create_subset_by_language ( self , language : str ) -> \"Corpus\" : \"\"\" Args: language (str): Returns: Corpus \"\"\" return Corpus ( sentences = ( subset := [ sent for sent in self . sentences if sent . language == language ] ), raw = \" \" . join ([ sent . raw for sent in subset ]), )","title":"create_subset_by_language"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.from_txt","text":"Parameters: path ( str ) \u2013 Returns: Corpus \u2013 Corpus Source code in cltrier_nlp/corpus/__init__.py 163 164 165 166 167 168 169 170 171 172 173 @classmethod def from_txt ( cls , path : str ) -> \"Corpus\" : \"\"\" Args: path (str): Returns: Corpus \"\"\" return cls ( raw = open ( path ) . read ())","title":"from_txt"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.generate_ngrams","text":"Parameters: n ( int ) \u2013 Returns: NGrams \u2013 utility.types.NGrams: Source code in cltrier_nlp/corpus/__init__.py 150 151 152 153 154 155 156 157 158 159 160 161 def generate_ngrams ( self , n : int ) -> utility . types . NGrams : \"\"\" Args: n (int): Returns: utility.types.NGrams: \"\"\" return [ ngram for sent in self . sentences for ngram in functional . text . ngrams ( sent . tokens , n ) ]","title":"generate_ngrams"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.model_post_init","text":"Parameters: __context ( Any ) \u2013 ?? Returns: Source code in cltrier_nlp/corpus/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @functional . timeit def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: \"\"\" if not self . sentences : self . sentences = [ Sentence ( raw = sent ) for sent in functional . text . sentenize ( self . raw ) ]","title":"model_post_init"},{"location":"corpus/#cltrier_nlp.corpus.Corpus.to_df","text":"Returns: DataFrame \u2013 pandas.DataFrame: Source code in cltrier_nlp/corpus/__init__.py 175 176 177 178 179 180 181 182 183 def to_df ( self ) -> pandas . DataFrame : \"\"\" Returns: pandas.DataFrame: \"\"\" return pandas . DataFrame ( [ sent . to_row () for sent in self . sentences ], )","title":"to_df"},{"location":"corpus/#cltrier_nlp.corpus.CorpusArgs","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/__init__.py 18 19 20 21 22 23 24 class CorpusArgs ( pydantic . BaseModel ): \"\"\" \"\"\" token_count_exclude : utility . types . Tokens = pydantic . Field ( default_factory = lambda : [ \"\u2019\" , \"\u201c\" , \"\u201d\" , * string . punctuation ] )","title":"CorpusArgs"},{"location":"corpus/#cltrier_nlp.corpus.Sentence","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"Sentence"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.bigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"bigrams"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.pentagram","text":"Returns:","title":"pentagram"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.tetragram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"tetragram"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.trigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"trigrams"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.__len__","text":"Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"__len__"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.model_post_init","text":"Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw )","title":"model_post_init"},{"location":"corpus/#cltrier_nlp.corpus.Sentence.to_row","text":"Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"to_row"},{"location":"corpus/sentence/","text":"Sentence Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) bigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: pentagram : typing . List [ typing . Tuple [ str , ... ]] property Returns: tetragram : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: trigrams : utility . types . NGrams property Returns: NGrams \u2013 utility.types.NGrams: __len__ () Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens ) model_post_init ( __context ) Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) to_row () Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"Sentence"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence","text":"Bases: BaseModel Source code in cltrier_nlp/corpus/sentence.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Sentence ( pydantic . BaseModel ): \"\"\" \"\"\" raw : str language : str = functional . text . UNK_LANG tokens : utility . types . Tokens = pydantic . Field ( default_factory = lambda : []) def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw ) @pydantic . computed_field # type: ignore[misc] @property def bigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 2 ) @pydantic . computed_field # type: ignore[misc] @property def trigrams ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 3 ) @pydantic . computed_field # type: ignore[misc] @property def tetragram ( self ) -> utility . types . NGrams : \"\"\" Returns: utility.types.NGrams: \"\"\" return functional . text . ngrams ( self . tokens , 4 ) @pydantic . computed_field # type: ignore[misc] @property def pentagram ( self ) -> typing . List [ typing . Tuple [ str , ... ]]: \"\"\" Returns: \"\"\" return functional . text . ngrams ( self . tokens , 5 ) def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ()) def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"Sentence"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.bigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"bigrams"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.pentagram","text":"Returns:","title":"pentagram"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.tetragram","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"tetragram"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.trigrams","text":"Returns: NGrams \u2013 utility.types.NGrams:","title":"trigrams"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.__len__","text":"Returns: int ( int ) \u2013 Source code in cltrier_nlp/corpus/sentence.py 88 89 90 91 92 93 94 95 def __len__ ( self ) -> int : \"\"\" Returns: int: \"\"\" return len ( self . tokens )","title":"__len__"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.model_post_init","text":"Parameters: __context ( Any ) \u2013 ?? Returns: None \u2013 None Source code in cltrier_nlp/corpus/sentence.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def model_post_init ( self , __context : typing . Any ) -> None : \"\"\" Args: __context (typing.Any): ?? Returns: None \"\"\" self . raw = self . raw . replace ( \" \\n \" , \" \" ) . strip () if self . language == functional . text . UNK_LANG : self . language = functional . text . detect_language ( self . raw ) if not self . tokens : self . tokens = functional . text . tokenize ( self . raw )","title":"model_post_init"},{"location":"corpus/sentence/#cltrier_nlp.corpus.sentence.Sentence.to_row","text":"Returns: Series \u2013 pandas.Series: Source code in cltrier_nlp/corpus/sentence.py 79 80 81 82 83 84 85 86 def to_row ( self ) -> pandas . Series : \"\"\" Returns: pandas.Series: \"\"\" return pandas . Series ( self . model_dump ())","title":"to_row"},{"location":"encoder/","text":"Encoder Bases: Module Source code in cltrier_nlp/encoder/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class Encoder ( torch . nn . Module ): \"\"\" \"\"\" @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), ** { \"embeds\" : self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), \"token\" : [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], \"unpad\" : unpad , } ) def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) @property def dim ( self ) -> int : \"\"\" Return the dimension of the model. \"\"\" return self . model . config . to_dict ()[ \"hidden_size\" ] def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" ) dim : int property Return the dimension of the model. __call__ ( batch , unpad = False ) Tokenizes input batch and returns embeddings and tokens with optional padding removal. Parameters: batch ( Batch [ str ] ) \u2013 List of input strings to be tokenized. unpad ( bool , default: False ) \u2013 Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch ( EncoderBatch ) \u2013 A EncodedBatch object. See documentation for more. Source code in cltrier_nlp/encoder/__init__.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), ** { \"embeds\" : self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), \"token\" : [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], \"unpad\" : unpad , } ) __init__ ( args = EncoderArgs ()) Initialize the encoder with the provided EncoderConfig. Parameters: args ( EncoderArgs , default: EncoderArgs () ) \u2013 The configuration for the encoder. Source code in cltrier_nlp/encoder/__init__.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) __len__ () Return the length of the object based on the vocabulary size. Source code in cltrier_nlp/encoder/__init__.py 131 132 133 134 135 def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] __repr__ () Return a string representation of the encoder including memory usage. Source code in cltrier_nlp/encoder/__init__.py 137 138 139 140 141 142 143 144 def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" ) forward ( ids , masks ) Perform a forward pass through the model and return the aggregated hidden states. Parameters: ids ( Tensor ) \u2013 The input tensor for token ids. masks ( Tensor ) \u2013 The input tensor for attention masks. Returns: Tensor \u2013 torch.Tensor: The aggregated hidden states obtained from the model's forward pass. Source code in cltrier_nlp/encoder/__init__.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) ids_to_sent ( ids ) Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Parameters: ids ( Tensor ) \u2013 The input tensor of token IDs. Returns: str ( str ) \u2013 The decoded string output. Source code in cltrier_nlp/encoder/__init__.py 112 113 114 115 116 117 118 119 120 121 122 def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) ids_to_tokens ( ids ) Convert the input token IDs to a list of token strings using the internal tokenizer. Parameters: ids ( Tensor ) \u2013 The input token IDs to be converted to tokens. Returns: List [ str ] \u2013 List[str]: A list of token strings corresponding to the input token IDs. Source code in cltrier_nlp/encoder/__init__.py 100 101 102 103 104 105 106 107 108 109 110 def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) EncoderArgs Bases: BaseModel Source code in cltrier_nlp/encoder/__init__.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderArgs ( pydantic . BaseModel ): \"\"\" \"\"\" model : str = \"prajjwal1/bert-tiny\" layers : typing . List [ int ] = [ - 1 ] device : str = functional . neural . get_device () tokenizer : typing . Dict [ str , str | int ] = dict ( max_length = 512 , truncation = True , return_offsets_mapping = True , ) EncoderBatch Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : utility . types . Batch [ torch . Tensor ] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )] EncoderPooler Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ]) __call__ ( encodes , extract_spans = None , form = EncoderPoolerArgs () . types ) Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"Encoder"},{"location":"encoder/#cltrier_nlp.encoder.Encoder","text":"Bases: Module Source code in cltrier_nlp/encoder/__init__.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class Encoder ( torch . nn . Module ): \"\"\" \"\"\" @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self ) def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), ** { \"embeds\" : self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), \"token\" : [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], \"unpad\" : unpad , } ) def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () ) def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids ) def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True ) @property def dim ( self ) -> int : \"\"\" Return the dimension of the model. \"\"\" return self . model . config . to_dict ()[ \"hidden_size\" ] def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ] def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" )","title":"Encoder"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.dim","text":"Return the dimension of the model.","title":"dim"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__call__","text":"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Parameters: batch ( Batch [ str ] ) \u2013 List of input strings to be tokenized. unpad ( bool , default: False ) \u2013 Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch ( EncoderBatch ) \u2013 A EncodedBatch object. See documentation for more. Source code in cltrier_nlp/encoder/__init__.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __call__ ( self , batch : utility . types . Batch [ str ], unpad : bool = False ) -> EncoderBatch : \"\"\"Tokenizes input batch and returns embeddings and tokens with optional padding removal. Args: batch (utility.types.Batch[str]): List of input strings to be tokenized. unpad (bool, optional): Whether to remove padding from embeddings and tokens. Defaults to True. Returns: EncoderBatch: A EncodedBatch object. See documentation for more. \"\"\" return EncoderBatch ( ** ( encoding := self . tokenizer ( batch , padding = True , ** self . args . tokenizer )), ** { \"embeds\" : self . forward ( torch . tensor ( encoding [ \"input_ids\" ], device = self . args . device ) . long (), torch . tensor ( encoding [ \"attention_mask\" ], device = self . args . device ) . short (), ), \"token\" : [ self . ids_to_tokens ( ids ) for ids in encoding [ \"input_ids\" ]], \"unpad\" : unpad , } )","title":"__call__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__init__","text":"Initialize the encoder with the provided EncoderConfig. Parameters: args ( EncoderArgs , default: EncoderArgs () ) \u2013 The configuration for the encoder. Source code in cltrier_nlp/encoder/__init__.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @functional . timeit def __init__ ( self , args : EncoderArgs = EncoderArgs ()): \"\"\"Initialize the encoder with the provided EncoderConfig. Args: args (EncoderArgs): The configuration for the encoder. \"\"\" super () . __init__ () self . args = args self . tokenizer = transformers . AutoTokenizer . from_pretrained ( args . model ) self . model = transformers . AutoModel . from_pretrained ( args . model , output_hidden_states = True ) . to ( self . args . device ) logging . info ( self )","title":"__init__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__len__","text":"Return the length of the object based on the vocabulary size. Source code in cltrier_nlp/encoder/__init__.py 131 132 133 134 135 def __len__ ( self ) -> int : \"\"\" Return the length of the object based on the vocabulary size. \"\"\" return self . model . config . to_dict ()[ \"vocab_size\" ]","title":"__len__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.__repr__","text":"Return a string representation of the encoder including memory usage. Source code in cltrier_nlp/encoder/__init__.py 137 138 139 140 141 142 143 144 def __repr__ ( self ) -> str : \"\"\" Return a string representation of the encoder including memory usage. \"\"\" return ( f '> Encoder Name: { self . model . config . __dict__ [ \"_name_or_path\" ] } \\n ' f \" Memory Usage: { functional . neural . calculate_model_memory_usage ( self . model ) } \" )","title":"__repr__"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.forward","text":"Perform a forward pass through the model and return the aggregated hidden states. Parameters: ids ( Tensor ) \u2013 The input tensor for token ids. masks ( Tensor ) \u2013 The input tensor for attention masks. Returns: Tensor \u2013 torch.Tensor: The aggregated hidden states obtained from the model's forward pass. Source code in cltrier_nlp/encoder/__init__.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def forward ( self , ids : torch . Tensor , masks : torch . Tensor ) -> torch . Tensor : \"\"\" Perform a forward pass through the model and return the aggregated hidden states. Args: ids (torch.Tensor): The input tensor for token ids. masks (torch.Tensor): The input tensor for attention masks. Returns: torch.Tensor: The aggregated hidden states obtained from the model's forward pass. \"\"\" return ( torch . stack ( [ self . model . forward ( ids , masks ) . hidden_states [ i ] for i in self . args . layers ] ) . sum ( 0 ) . squeeze () )","title":"forward"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.ids_to_sent","text":"Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Parameters: ids ( Tensor ) \u2013 The input tensor of token IDs. Returns: str ( str ) \u2013 The decoded string output. Source code in cltrier_nlp/encoder/__init__.py 112 113 114 115 116 117 118 119 120 121 122 def ids_to_sent ( self , ids : torch . Tensor ) -> str : \"\"\" Convert the input tensor of token IDs to a string using the internal tokenizers decode method. Args: ids (torch.Tensor): The input tensor of token IDs. Returns: str: The decoded string output. \"\"\" return self . tokenizer . decode ( ids , skip_special_tokens = True )","title":"ids_to_sent"},{"location":"encoder/#cltrier_nlp.encoder.Encoder.ids_to_tokens","text":"Convert the input token IDs to a list of token strings using the internal tokenizer. Parameters: ids ( Tensor ) \u2013 The input token IDs to be converted to tokens. Returns: List [ str ] \u2013 List[str]: A list of token strings corresponding to the input token IDs. Source code in cltrier_nlp/encoder/__init__.py 100 101 102 103 104 105 106 107 108 109 110 def ids_to_tokens ( self , ids : torch . Tensor ) -> typing . List [ str ]: \"\"\" Convert the input token IDs to a list of token strings using the internal tokenizer. Args: ids (torch.Tensor): The input token IDs to be converted to tokens. Returns: List[str]: A list of token strings corresponding to the input token IDs. \"\"\" return self . tokenizer . convert_ids_to_tokens ( ids )","title":"ids_to_tokens"},{"location":"encoder/#cltrier_nlp.encoder.EncoderArgs","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/__init__.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderArgs ( pydantic . BaseModel ): \"\"\" \"\"\" model : str = \"prajjwal1/bert-tiny\" layers : typing . List [ int ] = [ - 1 ] device : str = functional . neural . get_device () tokenizer : typing . Dict [ str , str | int ] = dict ( max_length = 512 , truncation = True , return_offsets_mapping = True , )","title":"EncoderArgs"},{"location":"encoder/#cltrier_nlp.encoder.EncoderBatch","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : utility . types . Batch [ torch . Tensor ] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"EncoderBatch"},{"location":"encoder/#cltrier_nlp.encoder.EncoderPooler","text":"Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ])","title":"EncoderPooler"},{"location":"encoder/#cltrier_nlp.encoder.EncoderPooler.__call__","text":"Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"__call__"},{"location":"encoder/batch/","text":"EncoderBatch Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : utility . types . Batch [ torch . Tensor ] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"Batch"},{"location":"encoder/batch/#cltrier_nlp.encoder.batch.EncoderBatch","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/batch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class EncoderBatch ( pydantic . BaseModel ): \"\"\" \"\"\" embeds : utility . types . Batch [ torch . Tensor ] token : utility . types . Batch [ utility . types . Tokens ] input_ids : utility . types . Batch [ typing . List [ int ]] token_type_ids : utility . types . Batch [ typing . List [ int ]] attention_mask : utility . types . Batch [ typing . List [ int ]] offset_mapping : utility . types . Batch [ typing . List [ typing . Tuple [ int , int ]]] unpad : bool = True model_config = pydantic . ConfigDict ( arbitrary_types_allowed = True ) def model_post_init ( self , __context ) -> None : if self . unpad : mask = torch . tensor ( self . attention_mask ) . sum ( 1 ) self . embeds = [ v [: n ] for v , n in zip ( self . embeds , mask )] self . token = [ v [: n ] for v , n in zip ( self . token , mask )]","title":"EncoderBatch"},{"location":"encoder/pooler/","text":"EncoderPooler Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ]) __call__ ( encodes , extract_spans = None , form = EncoderPoolerArgs () . types ) Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] EncoderPoolerArgs Bases: BaseModel Source code in cltrier_nlp/encoder/pooler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EncoderPoolerArgs ( pydantic . BaseModel ): \"\"\" \"\"\" fns : typing . Dict [ str , typing . Callable ] = { # sentence based \"sent_cls\" : lambda x : x [ 0 ], \"sent_mean\" : lambda x : torch . mean ( x [ 1 : - 1 ], dim = 0 ), # word based, positional extraction \"subword_first\" : lambda x : x [ 0 ], \"subword_last\" : lambda x : x [ - 1 ], # word based, arithmetic extraction \"subword_mean\" : lambda x : torch . mean ( x , dim = 0 ), \"subword_min\" : lambda x : torch . min ( x , dim = 0 )[ 0 ], \"subword_max\" : lambda x : torch . max ( x , dim = 0 )[ 0 ], } types : typing . Literal [ \"sent_cls\" , \"sent_mean\" , \"subword_first\" , \"subword_last\" , \"subword_mean\" , \"subword_min\" , \"subword_max\" , ] = \"sent_cls\"","title":"Pooler"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPooler","text":"Source code in cltrier_nlp/encoder/pooler.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class EncoderPooler : \"\"\" \"\"\" def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ] @staticmethod def _extract_embed_spans ( encodes : EncoderBatch , extract_spans ) -> typing . Generator : \"\"\" \"\"\" for span , mapping , embeds in zip ( extract_spans , encodes . offset_mapping , encodes . embeds ): emb_span_idx = EncoderPooler . _get_token_idx ( mapping [ 1 : embeds . size ( dim = 0 ) - 1 ], span ) yield embeds [ emb_span_idx [ 0 ] : emb_span_idx [ 1 ] + 1 ] @staticmethod def _get_token_idx ( mapping : typing . List [ typing . Tuple [ int , int ]], c_span : typing . Tuple [ int , int ] ) -> typing . Tuple [ int , int ]: \"\"\" \"\"\" def prep_map ( pos ): return list ( enumerate ( list ( zip ( * mapping ))[ pos ])) span : typing . Tuple [ int , int ] = ( next ( eid for eid , cid in reversed ( prep_map ( 0 )) if cid <= c_span [ 0 ]), next ( eid for eid , cid in prep_map ( 1 ) if cid >= c_span [ 1 ]), ) return span if span [ 0 ] <= span [ 1 ] else ( span [ 1 ], span [ 0 ])","title":"EncoderPooler"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPooler.__call__","text":"Source code in cltrier_nlp/encoder/pooler.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __call__ ( self , encodes : EncoderBatch , extract_spans : typing . Union [ typing . List [ typing . Tuple [ int , int ]], None ] = None , form = EncoderPoolerArgs () . types , ) -> typing . List [ torch . Tensor ]: \"\"\" \"\"\" if form not in [ \"sent_cls\" , \"sent_mean\" ] and not extract_spans : raise ValueError ( \"Please provide a list of span values to extract.\" ) return [ EncoderPoolerArgs () . fns [ form ]( embed ) for embed in ( encodes . embeds if form in [ \"sent_cls\" , \"sent_mean\" ] else EncoderPooler . _extract_embed_spans ( encodes , extract_spans ) ) ]","title":"__call__"},{"location":"encoder/pooler/#cltrier_nlp.encoder.pooler.EncoderPoolerArgs","text":"Bases: BaseModel Source code in cltrier_nlp/encoder/pooler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EncoderPoolerArgs ( pydantic . BaseModel ): \"\"\" \"\"\" fns : typing . Dict [ str , typing . Callable ] = { # sentence based \"sent_cls\" : lambda x : x [ 0 ], \"sent_mean\" : lambda x : torch . mean ( x [ 1 : - 1 ], dim = 0 ), # word based, positional extraction \"subword_first\" : lambda x : x [ 0 ], \"subword_last\" : lambda x : x [ - 1 ], # word based, arithmetic extraction \"subword_mean\" : lambda x : torch . mean ( x , dim = 0 ), \"subword_min\" : lambda x : torch . min ( x , dim = 0 )[ 0 ], \"subword_max\" : lambda x : torch . max ( x , dim = 0 )[ 0 ], } types : typing . Literal [ \"sent_cls\" , \"sent_mean\" , \"subword_first\" , \"subword_last\" , \"subword_mean\" , \"subword_min\" , \"subword_max\" , ] = \"sent_cls\"","title":"EncoderPoolerArgs"},{"location":"functional/","text":"This functional collection aims to improve code usability, readability, and overall development efficiency. They can be integrated into educational and research projects. timeit ( func ) Decorator function to measure the execution time of the function argument. Parameters: func ( Callable ) \u2013 The function to be measured. Returns: any ( Callable ) \u2013 The result of the input function. Source code in cltrier_nlp/functional/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def timeit ( func : typing . Callable ) -> typing . Callable : \"\"\" Decorator function to measure the execution time of the function argument. Args: func (Callable): The function to be measured. Returns: any: The result of the input function. \"\"\" @functools . wraps ( func ) def wrap ( * args , ** kwargs ) -> typing . Any : \"\"\" Decorator function that wraps the input function, measures its execution time, logs the result, and returns the result. \"\"\" start = time . time () result = func ( * args , ** kwargs ) logging . info ( f \"> f( { func . __name__ } ) took: { time . time () - start : 2.4f } sec\" ) return result return wrap","title":"Functional"},{"location":"functional/#cltrier_nlp.functional.timeit","text":"Decorator function to measure the execution time of the function argument. Parameters: func ( Callable ) \u2013 The function to be measured. Returns: any ( Callable ) \u2013 The result of the input function. Source code in cltrier_nlp/functional/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def timeit ( func : typing . Callable ) -> typing . Callable : \"\"\" Decorator function to measure the execution time of the function argument. Args: func (Callable): The function to be measured. Returns: any: The result of the input function. \"\"\" @functools . wraps ( func ) def wrap ( * args , ** kwargs ) -> typing . Any : \"\"\" Decorator function that wraps the input function, measures its execution time, logs the result, and returns the result. \"\"\" start = time . time () result = func ( * args , ** kwargs ) logging . info ( f \"> f( { func . __name__ } ) took: { time . time () - start : 2.4f } sec\" ) return result return wrap","title":"timeit"},{"location":"functional/neural/","text":"calculate_model_memory_usage ( model ) Calculate the memory usage of the input model in megabytes. Parameters: model ( Module ) \u2013 The input model for which memory usage needs to be calculated. Returns: str ( str ) \u2013 A string formatted to represent the size of the nn.Module in megabytes. Source code in cltrier_nlp/functional/neural.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def calculate_model_memory_usage ( model : torch . nn . Module ) -> str : \"\"\"Calculate the memory usage of the input model in megabytes. Args: model (torch.nn.Module): The input model for which memory usage needs to be calculated. Returns: str: A string formatted to represent the size of the nn.Module in megabytes. \"\"\" usage_in_byte : int = sum ( [ sum ([ param . nelement () * param . element_size () for param in model . parameters ()]), sum ([ buf . nelement () * buf . element_size () for buf in model . buffers ()]), ] ) return f \" { usage_in_byte / ( 1024.0 * 1024.0 ) : 2.4f } MB\" get_device () Return the computation device as a string based on the availability of CUDA. Returns: str ( str ) \u2013 A PyTorch device object representing the appropriate device for computation. Source code in cltrier_nlp/functional/neural.py 4 5 6 7 8 9 10 def get_device () -> str : \"\"\"Return the computation device as a string based on the availability of CUDA. Returns: str: A PyTorch device object representing the appropriate device for computation. \"\"\" return \"cuda\" if torch . cuda . is_available () else \"cpu\"","title":"Neural"},{"location":"functional/neural/#cltrier_nlp.functional.neural.calculate_model_memory_usage","text":"Calculate the memory usage of the input model in megabytes. Parameters: model ( Module ) \u2013 The input model for which memory usage needs to be calculated. Returns: str ( str ) \u2013 A string formatted to represent the size of the nn.Module in megabytes. Source code in cltrier_nlp/functional/neural.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def calculate_model_memory_usage ( model : torch . nn . Module ) -> str : \"\"\"Calculate the memory usage of the input model in megabytes. Args: model (torch.nn.Module): The input model for which memory usage needs to be calculated. Returns: str: A string formatted to represent the size of the nn.Module in megabytes. \"\"\" usage_in_byte : int = sum ( [ sum ([ param . nelement () * param . element_size () for param in model . parameters ()]), sum ([ buf . nelement () * buf . element_size () for buf in model . buffers ()]), ] ) return f \" { usage_in_byte / ( 1024.0 * 1024.0 ) : 2.4f } MB\"","title":"calculate_model_memory_usage"},{"location":"functional/neural/#cltrier_nlp.functional.neural.get_device","text":"Return the computation device as a string based on the availability of CUDA. Returns: str ( str ) \u2013 A PyTorch device object representing the appropriate device for computation. Source code in cltrier_nlp/functional/neural.py 4 5 6 7 8 9 10 def get_device () -> str : \"\"\"Return the computation device as a string based on the availability of CUDA. Returns: str: A PyTorch device object representing the appropriate device for computation. \"\"\" return \"cuda\" if torch . cuda . is_available () else \"cpu\"","title":"get_device"},{"location":"functional/text/","text":"detect_language ( content ) Parameters: content ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def detect_language ( content : str ) -> str : \"\"\" Args: content: Returns: \"\"\" # Ignore langcodes dependent language data warning # DeprecationWarning: pkg_resources is deprecated as an API. with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) try : return langcodes . Language . get ( langdetect . detect ( content )) . display_name () . lower () except langdetect . lang_detect_exception . LangDetectException : return UNK_LANG load_stopwords ( languages ) Parameters: languages ( Tokens ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def load_stopwords ( languages : utility . types . Tokens ) -> utility . types . Tokens : \"\"\" Args: languages: Returns: \"\"\" return list ( set () . union ( * [ nltk . corpus . stopwords . words ( lang ) for lang in languages if lang in nltk . corpus . stopwords . fileids () ] ) ) ngrams ( tokens , n ) Parameters: tokens ( Tokens ) \u2013 n ( int ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 60 61 62 63 64 65 66 67 68 69 70 def ngrams ( tokens : utility . types . Tokens , n : int ) -> utility . types . NGrams : \"\"\" Args: tokens: n: Returns: \"\"\" return [ tuple ( tokens [ i : i + n ]) for i in range ( len ( tokens ) - n + 1 )] sentenize ( text ) Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 32 33 34 35 36 37 38 39 40 41 def sentenize ( text : str ) -> utility . types . Batch [ str ]: \"\"\" Args: text: Returns: \"\"\" return nltk . tokenize . sent_tokenize ( text , language = detect_language ( text )) tokenize ( text ) Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def tokenize ( text : str ) -> utility . types . Tokens : \"\"\" Args: text: Returns: \"\"\" try : return nltk . tokenize . word_tokenize ( text . lower (), language = detect_language ( text )) except LookupError : return nltk . tokenize . word_tokenize ( text . lower ())","title":"Text"},{"location":"functional/text/#cltrier_nlp.functional.text.detect_language","text":"Parameters: content ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def detect_language ( content : str ) -> str : \"\"\" Args: content: Returns: \"\"\" # Ignore langcodes dependent language data warning # DeprecationWarning: pkg_resources is deprecated as an API. with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) try : return langcodes . Language . get ( langdetect . detect ( content )) . display_name () . lower () except langdetect . lang_detect_exception . LangDetectException : return UNK_LANG","title":"detect_language"},{"location":"functional/text/#cltrier_nlp.functional.text.load_stopwords","text":"Parameters: languages ( Tokens ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def load_stopwords ( languages : utility . types . Tokens ) -> utility . types . Tokens : \"\"\" Args: languages: Returns: \"\"\" return list ( set () . union ( * [ nltk . corpus . stopwords . words ( lang ) for lang in languages if lang in nltk . corpus . stopwords . fileids () ] ) )","title":"load_stopwords"},{"location":"functional/text/#cltrier_nlp.functional.text.ngrams","text":"Parameters: tokens ( Tokens ) \u2013 n ( int ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 60 61 62 63 64 65 66 67 68 69 70 def ngrams ( tokens : utility . types . Tokens , n : int ) -> utility . types . NGrams : \"\"\" Args: tokens: n: Returns: \"\"\" return [ tuple ( tokens [ i : i + n ]) for i in range ( len ( tokens ) - n + 1 )]","title":"ngrams"},{"location":"functional/text/#cltrier_nlp.functional.text.sentenize","text":"Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 32 33 34 35 36 37 38 39 40 41 def sentenize ( text : str ) -> utility . types . Batch [ str ]: \"\"\" Args: text: Returns: \"\"\" return nltk . tokenize . sent_tokenize ( text , language = detect_language ( text ))","title":"sentenize"},{"location":"functional/text/#cltrier_nlp.functional.text.tokenize","text":"Parameters: text ( str ) \u2013 Returns: Source code in cltrier_nlp/functional/text.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def tokenize ( text : str ) -> utility . types . Tokens : \"\"\" Args: text: Returns: \"\"\" try : return nltk . tokenize . word_tokenize ( text . lower (), language = detect_language ( text )) except LookupError : return nltk . tokenize . word_tokenize ( text . lower ())","title":"tokenize"},{"location":"utility/","text":"Map Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) __init__ ( keys ) Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} __len__ () Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) get_ids ( keys ) Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] get_keys ( ids ) Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"Utility"},{"location":"utility/#cltrier_nlp.utility.Map","text":"Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"Map"},{"location":"utility/#cltrier_nlp.utility.Map.__init__","text":"Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()}","title":"__init__"},{"location":"utility/#cltrier_nlp.utility.Map.__len__","text":"Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"__len__"},{"location":"utility/#cltrier_nlp.utility.Map.get_ids","text":"Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ]","title":"get_ids"},{"location":"utility/#cltrier_nlp.utility.Map.get_keys","text":"Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"get_keys"},{"location":"utility/map/","text":"Map Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) __init__ ( keys ) Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} __len__ () Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids ) get_ids ( keys ) Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] get_keys ( ids ) Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"Map"},{"location":"utility/map/#cltrier_nlp.utility.map.Map","text":"Source code in cltrier_nlp/utility/map.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Map : \"\"\" \"\"\" def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()} def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ] def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ] def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"Map"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.__init__","text":"Source code in cltrier_nlp/utility/map.py 9 10 11 12 13 14 def __init__ ( self , keys : typing . Set [ str ]) -> None : \"\"\" \"\"\" self . keys2ids : typing . Dict [ str , int ] = { key : idx for idx , key in enumerate ( keys )} self . ids2keys : typing . Dict [ int , str ] = { idx : key for key , idx in self . keys2ids . items ()}","title":"__init__"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.__len__","text":"Source code in cltrier_nlp/utility/map.py 28 29 30 31 32 def __len__ ( self ): \"\"\" \"\"\" return len ( self . keys2ids )","title":"__len__"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.get_ids","text":"Source code in cltrier_nlp/utility/map.py 16 17 18 19 20 def get_ids ( self , keys : typing . List [ str ]) -> typing . List [ int ]: \"\"\" \"\"\" return [ self . keys2ids [ key ] for key in keys ]","title":"get_ids"},{"location":"utility/map/#cltrier_nlp.utility.map.Map.get_keys","text":"Source code in cltrier_nlp/utility/map.py 22 23 24 25 26 def get_keys ( self , ids : typing . List [ int ]) -> typing . List [ str ]: \"\"\" \"\"\" return [ self . ids2keys [ idx ] for idx in ids ]","title":"get_keys"},{"location":"utility/types/","text":"","title":"Types"}]}